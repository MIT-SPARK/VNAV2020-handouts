<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><title>Exercises - VNAV2020</title><link rel="shortcut icon" href="https://mit-spark.github.io/VNAV2020-handouts/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="https://mit-spark.github.io/VNAV2020-handouts/assets/css/just-the-docs.css"> <script src="https://use.fontawesome.com/d81b2d50d8.js"></script> <script type="text/javascript" src="https://mit-spark.github.io/VNAV2020-handouts/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="https://mit-spark.github.io/VNAV2020-handouts/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Exercises | VNAV2020</title><meta name="generator" content="Jekyll v3.8.6" /><meta property="og:title" content="Exercises" /><meta property="og:locale" content="en_US" /><meta name="description" content="16.485 - Visual Navigation for Autonomous Vehicles (2020)" /><meta property="og:description" content="16.485 - Visual Navigation for Autonomous Vehicles (2020)" /><link rel="canonical" href="https://mit-spark.github.io/VNAV2020-handouts/lab6/exercises" /><meta property="og:url" content="https://mit-spark.github.io/VNAV2020-handouts/lab6/exercises" /><meta property="og:site_name" content="VNAV2020" /> <script type="application/ld+json"> {"@type":"WebPage","url":"https://mit-spark.github.io/VNAV2020-handouts/lab6/exercises","headline":"Exercises","description":"16.485 - Visual Navigation for Autonomous Vehicles (2020)","@context":"https://schema.org"}</script> <script> window.MathJax = { options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], }, loader: { load: ['input/tex', '[tex]/ams', '[tex]/configMacros'] }, tex: { packages: {'[+]': ['boldsymbol', 'ams', 'configMacros']}, inlineMath: [['$','$'], ['\\(','\\)']], displayMath: [["$$", "$$"], ["\\[","\\]"]], processEscapes: true, tags: "ams", macros: { SE: ['\\mathrm{SE}(#1)', 1], SO: ['\\mathrm{SO}(#1)', 1], argmin: '\\mathop{\\operatorname{argmin}}', argmax: '\\mathop{\\operatorname{argmax}}', trace: '\\operatorname{trace}', tran: '^{\\mathsf{T}}', }, }, }; </script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script> <script type="text/javascript" src="https://mit-spark.github.io/VNAV2020-handouts/assets/js/vendor/lazysizes.min.js" async=""></script><body><div class="page-wrap">
<div class="side-bar">
<div class="site-header"> <a href="https://mit-spark.github.io/VNAV2020-handouts/" class="site-title lh-tight">VNAV2020</a> <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button>
</div>
<div class="navigation main-nav js-main-nav"><nav role="navigation" aria-label="Main navigation"><ul class="navigation-list">
<li class="navigation-list-item"><a href="https://mit-spark.github.io/VNAV2020-handouts/" class="navigation-list-link">Home</a></li>
<li class="navigation-list-item">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/" class="navigation-list-link">Lab 1</a><ul class="navigation-list-child-list ">
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/ubuntu" class="navigation-list-link">Install Ubuntu 18.04</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/shell" class="navigation-list-link">Shell basics</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/git" class="navigation-list-link">Git</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/cpp" class="navigation-list-link">C++</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab1/exercises" class="navigation-list-link">Exercises</a></li>
</ul>
</li>
<li class="navigation-list-item">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab2/" class="navigation-list-link">Lab 2</a><ul class="navigation-list-child-list ">
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab2/ros" class="navigation-list-link">Installing ROS</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab2/ros101" class="navigation-list-link">Introduction to ROS</a></li>
<li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab2/exercises" class="navigation-list-link">Exercises</a></li>
</ul>
</li>
<li class="navigation-list-item">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab3/" class="navigation-list-link">Lab 3</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab3/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab4/" class="navigation-list-link">Lab 4</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab4/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab5/" class="navigation-list-link">Lab 5</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="https://mit-spark.github.io/VNAV2020-handouts/lab5/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item active">
<a href="https://mit-spark.github.io/VNAV2020-handouts/lab6/" class="navigation-list-link">Lab 6</a><ul class="navigation-list-child-list "><li class="navigation-list-item active"><a href="https://mit-spark.github.io/VNAV2020-handouts/lab6/exercises" class="navigation-list-link active">Exercises</a></li></ul>
</li>
<li class="navigation-list-item"><a href="https://mit-spark.github.io/VNAV2020-handouts/about" class="navigation-list-link">How to print</a></li>
</ul></nav></div>
<footer class="site-footer"><p class="text-small text-grey-dk-000 mb-4"><b>Last modified</b>:<br> Wednesday, October 7 at 15:07</p></footer>
</div>
<div class="main-content-wrap js-main-content" tabindex="0"><div class="main-content">
<div class="page-header js-page-header"><div class="search">
<div class="search-input-wrap"> <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search VNAV2020" aria-label="Search VNAV2020" autocomplete="off"> <svg width="14" height="14" viewbox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title>
<g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"></path><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"></path></g></svg>
</div>
<div class="js-search-results search-results-wrap"></div>
</div></div>
<div class="page">
<nav class="breadcrumb-nav"><ol class="breadcrumb-nav-list">
<li class="breadcrumb-nav-list-item"><a href="https://mit-spark.github.io/VNAV2020-handouts/lab6/">Lab 6</a></li>
<li class="breadcrumb-nav-list-item"><span>Exercises</span></li>
</ol></nav><div id="main-content" class="page-content" role="main">
<h1 class="no_toc text-delta fs-9" id="exercises"> <a href="#exercises" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Exercises</h1>
<ol id="markdown-toc">
<li>
<a href="#submission" id="markdown-toc-submission">Submission</a><ol>
<li><a href="#individual" id="markdown-toc-individual">Individual</a></li>
<li><a href="#team" id="markdown-toc-team">Team</a></li>
<li><a href="#deadline" id="markdown-toc-deadline">Deadline</a></li>
</ol>
</li>
<li>
<a href="#-individual" id="markdown-toc--individual">👤 Individual</a><ol>
<li>
<a href="#-deliverable-1---nisters-5-point-algorithm-20-pts" id="markdown-toc--deliverable-1---nisters-5-point-algorithm-20-pts">📨 Deliverable 1 - Nister’s 5-point Algorithm [20 pts]</a><ol><li><a href="#read-the-paper-and-answer-the-questions-below" id="markdown-toc-read-the-paper-and-answer-the-questions-below">Read the paper and answer the questions below</a></li></ol>
</li>
<li><a href="#-deliverable-2---designing-a-minimal-solver-10-pts" id="markdown-toc--deliverable-2---designing-a-minimal-solver-10-pts">📨 Deliverable 2 - Designing a Minimal Solver [10 pts]</a></li>
</ol>
</li>
<li>
<a href="#-team" id="markdown-toc--team">👥 Team</a><ol>
<li><a href="#getting-started-code-base-and-datasets" id="markdown-toc-getting-started-code-base-and-datasets">Getting started: code base and datasets</a></li>
<li><a href="#lets-perform-motion-estimation" id="markdown-toc-lets-perform-motion-estimation">Let’s perform motion estimation!</a></li>
<li><a href="#-deliverable-3---initial-setup-10-pts" id="markdown-toc--deliverable-3---initial-setup-10-pts">📨 Deliverable 3 - Initial Setup [10 pts]</a></li>
<li>
<a href="#-deliverable-4---2d-2d-correspondences-40-pts" id="markdown-toc--deliverable-4---2d-2d-correspondences-40-pts">📨 Deliverable 4 - 2D-2D Correspondences [40 pts]</a><ol>
<li><a href="#1-cameracallback-this-is-the-main-function-for-this-lab" id="markdown-toc-1-cameracallback-this-is-the-main-function-for-this-lab">1. <code class="highlighter-rouge">cameraCallback</code>: this is the main function for this lab.</a></li>
<li><a href="#2-evaluaterpe-evaluating-the-relative-pose-estimates" id="markdown-toc-2-evaluaterpe-evaluating-the-relative-pose-estimates">2. <code class="highlighter-rouge">evaluateRPE</code>: evaluating the relative pose estimates</a></li>
<li><a href="#3-publish-your-relative-pose-estimate" id="markdown-toc-3-publish-your-relative-pose-estimate">3. Publish your relative pose estimate</a></li>
</ol>
</li>
<li>
<a href="#-deliverable-5---3d-3d-correspondences-20-pts" id="markdown-toc--deliverable-5---3d-3d-correspondences-20-pts">📨 Deliverable 5 - 3D-3D Correspondences [20 pts]</a><ol><li><a href="#1-cameracallback-implement-aruns-algorithm" id="markdown-toc-1-cameracallback-implement-aruns-algorithm">1. <code class="highlighter-rouge">cameraCallback</code>: Implement Arun’s algorithm</a></li></ol>
</li>
<li><a href="#summary-of-team-deliverables" id="markdown-toc-summary-of-team-deliverables">Summary of Team Deliverables</a></li>
<li><a href="#-optional-deliverable-6---3d-2d-correspondences-20-pts" id="markdown-toc--optional-deliverable-6---3d-2d-correspondences-20-pts">📨 [Optional] Deliverable 6 - 3D-2D Correspondences [20 pts]</a></li>
</ol>
</li>
</ol>
<h1 id="submission"> <a href="#submission" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Submission</h1>
<p>In this Lab, there are 5 deliverables throughout the handout. Deliverables 1 and 2 will require pen and paper and are considered individual tasks, while Deliverable 3-5 are a team task which requires coding in the lab6 directory that we will provide.</p>
<h3 id="individual"> <a href="#individual" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Individual</h3>
<p>Please push the deliverables into your personal repository, for math-related questions LaTeX (or other typesetting software) is required.</p>
<h3 id="team"> <a href="#team" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Team</h3>
<p>Please push the source code of the entire <code class="highlighter-rouge">lab6</code> package in the folder <code class="highlighter-rouge">lab6</code> of the team repository. Include also in your <code class="highlighter-rouge">lab6</code> folder a PDF containing non-code deliverables (plots, comments).</p>
<h3 id="deadline"> <a href="#deadline" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Deadline</h3>
<p><strong>Deadline:</strong> the VNAV staff will clone your repository on <strong>October 14</strong> at midnight ET.</p>
<h1 id="-individual"> <a href="#-individual" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 👤 Individual</h1>
<h2 id="-deliverable-1---nisters-5-point-algorithm-20-pts"> <a href="#-deliverable-1---nisters-5-point-algorithm-20-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 Deliverable 1 - Nister’s 5-point Algorithm [20 pts]</h2>
<h3 id="read-the-paper-and-answer-the-questions-below"> <a href="#read-the-paper-and-answer-the-questions-below" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Read the paper and answer the questions below</h3>
<p>Read the following paper.</p>
<p>[1] Nistér, David. “An efficient solution to the five-point relative pose problem.” 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Vol. 2. 2003. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.8769&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener noreferrer">link here</a>.</p>
<p>Questions:</p>
<ol>
<li>Outline the main computational steps required to get the relative pose estimate (up to scale) in Nister’s 5-point algorithm.</li>
<li>Does the 5-point algorithm exhibit any degeneracy? (degeneracy = special arrangements of the 3D points or the camera poses under which the algorithm fails)</li>
<li>When used within RANSAC, what is the expected number of iterations the 5-point algorithm requires to find an outlier-free set?<ul><li class="hint">Hint: take same assumptions of the lecture notes</li></ul>
</li>
</ol>
<h2 id="-deliverable-2---designing-a-minimal-solver-10-pts"> <a href="#-deliverable-2---designing-a-minimal-solver-10-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 Deliverable 2 - Designing a Minimal Solver [10 pts]</h2>
<p><strong>Can you do better than Nister?</strong> Nister’s method is a minimal solver since it uses 5 point correspondences to compute the 5 degrees of freedom that define the relative pose (up to scale) between the two cameras (recall: each point induces a scalar equation). In presence of external information (e.g., data from other sensors), we may be able use less point correspondences to compute the relative pose.</p>
<p>Consider a drone flying in an unknown environment, and equipped with a camera and an Inertial Measurement Unit (IMU). We want to use the feature correspondences extracted in the images captured at two consecutive time instants $t_1$ and $t_2$ to estimate the elative pose (up to scale) between the pose at time $t_1$ and the pose at time $t_2$. Besides the camera, we can use the IMU (and in particular the gyroscopes in the IMU) to estimate the relative rotation between the pose of the camera at time $t_1$ and $t_2$.</p>
<p>You are required to solve the following problems:</p>
<ol>
<li>Assume the relative camera rotation between time and is known from the IMU. Design a minimal solver that computes the remaining degrees of freedom of the relative pose.<ul><li class="hint">Hint: we only want to compute the pose up to scale</li></ul>
</li>
<li>OPTIONAL (5 bonus pts): Describe the pseudo-code of a RANSAC algorithm using the minimal solver developed in point a) to compute the relative pose in presence of outliers (wrong correspondences).</li>
</ol>
<h1 id="-team"> <a href="#-team" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 👥 Team</h1>
<p>In this section, we will estimate the motion of a flying drone in real time and compare the performances of different algorithms.</p>
<p>For the algorithms, we will be using the implementations provided in the <a href="https://laurentkneip.github.io/opengv/page_how_to_use.html" target="_blank" rel="noopener noreferrer">OpenGV</a> library (note: Open<strong>G</strong>V).</p>
<p>For the datasets, we will use pre-recorded <code class="highlighter-rouge">rosbag</code> files of an Intel Aero drone flying in an indoor environment (the high bay area of building 31 at MIT).</p>
<p>Additionally, for motion estimation:</p>
<ul>
<li>We will only focus on two-view (vs multi-camera) pose estimation. In OpenGV, we refer to two-view problems as “Central” (vs “Non-Central”) relative pose problems.</li>
<li>We will focus only on the calibrated case, where the matrix K is given, and we assume that the images are rectified (distortion removed) using the parameters that you estimated previously.</li>
</ul>
<h2 id="getting-started-code-base-and-datasets"> <a href="#getting-started-code-base-and-datasets" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Getting started: code base and datasets</h2>
<ul>
<li><p><strong>Prerequisites</strong>: Lab 6 will use the feature tracking algorithms developed in Lab 5, so make sure you have a working version of Lab 5 already in the VNAV workspace.</p></li>
<li>
<strong>Prepare the code base</strong>: Use <code class="highlighter-rouge">git pull</code> to update the git repo used to distribute lab codes (https://github.mit.edu/VNAV2020/labs), and you should see a new folder named <code class="highlighter-rouge">lab6</code>. Copy the entire <code class="highlighter-rouge">lab6</code> folder to the <code class="highlighter-rouge">src</code> folder of your vnav workspace (e.g., <code class="highlighter-rouge">~/vnav_ws/src</code>). Now we are ready to install OpenGV by doing:<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/vnav_ws/src <span class="o">(</span>path to src of vnav workspace<span class="o">)</span>
wstool init
wstool merge lab6/install/lab_6.rosinstall <span class="nt">-y</span>
wstool update <span class="nt">-j8</span>
</code></pre></div></div>
<p>The above scripts will download OpenGV into your workspace (you will see a folder <code class="highlighter-rouge">opengv</code> under <code class="highlighter-rouge">src</code>). Now run:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>catkin build lab_6
</code></pre></div></div>
<p>to build the <code class="highlighter-rouge">lab_6</code> package (which should build OpenGV first and then build <code class="highlighter-rouge">lab_6</code> itself).</p>
</li>
<li>
<strong>Download the datasets</strong>: We will use two datasets for this lab:<ol>
<li>Default: <code class="highlighter-rouge">2018-10-09-15-44-18.bag</code> and you can download it <a href="https://www.dropbox.com/s/hz8loviqgpjlk7k/2018-10-09-15-44-18.bag?dl=0" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>Lissajous: <code class="highlighter-rouge">lissajous_3d_2019-10-08-10-23-12.bag</code> and you can download it <a href="https://www.dropbox.com/s/p8ne6f6jgim6x4u/lissajous_3d_2019-10-08-10-23-12.bag?dl=0" target="_blank" rel="noopener noreferrer">here</a>. After downloading these two datasets, we suggest you to put them in the <code class="highlighter-rouge">~/data/vnav</code> folder.</li>
</ol>
</li>
</ul>
<p>The rosbag files includes the following topics of the drone:</p>
<ul>
<li>Ground-truth pose estimate of the drone: <code class="highlighter-rouge">/mavros/local_position/pose</code>
</li>
<li>Camera parameters: <code class="highlighter-rouge">/camera/rgb/camera_info</code>
</li>
<li>RGB image from the front camera of the drone: <code class="highlighter-rouge">/camera/rgb/image_raw</code>
</li>
<li>Rectified RGB camera image: <code class="highlighter-rouge">/camera/rgb/image_rect_color</code>
</li>
<li>Depth image: <code class="highlighter-rouge">/camera/depth/image_raw</code>
</li>
<li>Rectified depth image: <code class="highlighter-rouge">/camera/depth_registered/sw_registered/image_rect_raw</code>
</li>
<li>IMU data: <code class="highlighter-rouge">/mavros/imu/data_raw</code>
</li>
</ul>
<p>You can play these datasets by running:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rosbag play ~/data/vnav/2018-10-09-15-44-18.bag
</code></pre></div></div>
<p>while in parallel open RVIZ by:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rviz <span class="nt">-d</span> ~/vnav_ws/src/lab6/rviz/config.rviz
</code></pre></div></div>
<p>You should see on the left the RGB Image, the Depth image, and a Red arrow in the center window representing the ground-truth pose of the drone.</p>
<p>The difference between there two datasets is that the trajectory of the drone in the Lissajous dataset is more aggressive than the trajectory of the drone in the dafault dataset.</p>
<h2 id="lets-perform-motion-estimation"> <a href="#lets-perform-motion-estimation" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Let’s perform motion estimation!</h2>
<p>We will use two methods to estimate the motion of the drone:</p>
<ul>
<li>Motion estimation from 2D-2D correspondences (Deliverable 4)</li>
<li>Motion estimation from 3D-3D correspondences (Deliverable 5)</li>
</ul>
<p>In Deliverable 4, we will perform motion estimation <strong>only</strong> using 2D RGB images taken from the drone’s camera, while in Deliverable 5, we will additionally use the depth measurements to get the sense of 3D.</p>
<p><strong>NOTE:</strong></p>
<ul>
<li>As a general rule, you can add your own functions, classes, structures, callbacks, etc. that you might find suitable to generate the output we expect. We actually encourage you to build your own code! **For this lab, we provide a number of useful utility functions in <code class="highlighter-rouge">lab6_utils.h</code>. You do not need to use these functions to complete the assignment, but they might help save you some time and frustration.</li>
<li>All your main implementations of the motion estimation algorithms should be in the <code class="highlighter-rouge">pose_estimation.cpp</code> file. In the file, we have also provided many comments to help your implementation, so please go through the comments in details.</li>
</ul>
<h2 id="-deliverable-3---initial-setup-10-pts"> <a href="#-deliverable-3---initial-setup-10-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 Deliverable 3 - Initial Setup [10 pts]</h2>
<p>Before we go to motion estimation, an important task is to calibrate the camera of the drone, i.e., to obtain the camera intrinsics and distortion coefficients. Normally you would need to calibrate the camera yourself offline (or online, as we will discuss later in the course!) to obtain the parameters. The camera that the drone is equipped with, however, has been calibrated already, and calibration information is provided for you!</p>
<p>Therefore, to obtain the camera parameters and use it to rectify image keypoints. You’ll need to implement two functions:</p>
<ol>
<li>
<code class="highlighter-rouge">camInfoCallback</code>: This function grabs the camera parameters from camera info messages and populates the camera matrix</li>
<li>
<code class="highlighter-rouge">calibrateKeypoints</code>: This function rectifies the keypoints using the camera parameters you stored from <code class="highlighter-rouge">camInfoCallback</code>. You’ll want to use the function <code class="highlighter-rouge">cv::undistortPoints</code>.</li>
</ol>
<h2 id="-deliverable-4---2d-2d-correspondences-40-pts"> <a href="#-deliverable-4---2d-2d-correspondences-40-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 Deliverable 4 - 2D-2D Correspondences [40 pts]</h2>
<p>Given a set of keypoint correspondences in a pair of images (2D - 2D image correspondences), as computed in the previous lab 5, we can use 2-view (geometric verification) algorithms to estimate the relative pose (up to scale) from one viewpoint to another.</p>
<p>To do so, we will be using three different algorithms and comparing their performance.</p>
<p>We will first start with the 5-point algorithm of Nister. For this, we need to use the feature trackers we developed previously (use the provided solution code for lab 5 if you prefer). In particular, let us use SIFT for the remaining of the problem set.</p>
<p>You will need to track features using the code from Lab 5 to implement this part. You may use our provided solution code, or your own implementation if you choose.</p>
<p>We provide you with a skeleton code in <code class="highlighter-rouge">lab6</code> folder where we have set-up ROS callbacks to receive the necessary information.</p>
<p>We ask you to complete the code inside the following functions:</p>
<h3 id="1-cameracallback-this-is-the-main-function-for-this-lab"> <a href="#1-cameracallback-this-is-the-main-function-for-this-lab" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 1. <code class="highlighter-rouge">cameraCallback</code>: this is the main function for this lab.</h3>
<p>Inside, you will have to use three different algorithms to estimate the relative pose from frame to frame:</p>
<ul>
<li>the 5-point algorithm to estimate the relative pose from frame to frame, first <a href="https://laurentkneip.github.io/opengv/namespaceopengv_1_1relative__pose.html#af269f7393720263895fb9b746e4cec4a" target="_blank" rel="noopener noreferrer">without RANSAC (see OpenGV API)</a>, then <a href="https://laurentkneip.github.io/opengv/classopengv_1_1sac__problems_1_1relative__pose_1_1CentralRelativePoseSacProblem.html" target="_blank" rel="noopener noreferrer">with RANSAC (see OpenGV API)</a>
</li>
<li>OpenGV’s 8-point algorithm by Longuet-Higgins <a href="https://laurentkneip.github.io/opengv/namespaceopengv_1_1relative__pose.html#aebfc0d783d1dcadab2028e0b7ccfb528" target="_blank" rel="noopener noreferrer">without RANSAC</a> and <a href="https://laurentkneip.github.io/opengv/classopengv_1_1sac__problems_1_1relative__pose_1_1CentralRelativePoseSacProblem.html" target="_blank" rel="noopener noreferrer">with RANSAC</a>
</li>
<li>OpenGV’s 2-point algorithm. This algorithm requires you to provide the relative rotation between pairs of frames. This is usually done by integrating the IMU’s gyroscope measurements. Nevertheless, for this lab, we will ask you to compute the relative rotation using the ground-truth pose of the drone between both frames.</li>
</ul>
<p>For each part, follow the comments written in the source code for further details.</p>
<h3 id="2-evaluaterpe-evaluating-the-relative-pose-estimates"> <a href="#2-evaluaterpe-evaluating-the-relative-pose-estimates" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 2. <code class="highlighter-rouge">evaluateRPE</code>: evaluating the relative pose estimates</h3>
<p>We also want to evaluate the quality of the pose estimate for the given algorithms. Since you also have the ground-truth pose of the drone, it is possible to compute the Relative Pose Error between your estimated relative pose from frame to frame with the actual ground-truth movement. Follow the equations below (also given at the bottom of the README.md of this lab) and compute the Root Mean Squared Error for the RPE in translation and rotation for all consecutive frames in the rosbag provided.</p>
<p>Note that one of the poses is up to a scale! So you might want to normalize the ground-truth pose for fair comparison.</p>
<p>Also, <strong>provide 2 plots showing the rotation error and the translation error over time</strong> for each of the tested techniques (2 plots with 3 lines fro the algorithms without using RANSAC, and 3 other lines for the algorithms when using RANSAC). You can write the data to a file and do the plotting with Python if you prefer (upload as well the python script if necessary).</p>
<p><strong><em>The relative pose error is a metric for investigating the local consistency of a trajectory</em></strong></p>
<p>RPE compares the relative poses along the estimated and the reference trajectory:</p>
<p>\[ E_{i.j} = (P_{ref,i}^{-1}P_{ref,j})^{-1}(P_{est,i}^{-1}P_{est,j}) \in \SE{3} \]</p>
<p>where $P_{ref,i}^{-1}P_{ref,j} \in \SE{3}$ is the relative pose between the $i$-th frame and the $j$-th frame in the reference trajectory (in our case the ground-truth trajectory), and $P_{est,i}^{-1}P_{est,j} \in \SE{3}$ is the relative pose between the $i$-th frame and the $j$-th frame in the estimated trajectory (in our case the trajectory estimated from vision-based algorithms).</p>
<p>You can use different pose relations to calculate the RPE from timestamp $i$ to $j$.</p>
<p><strong>Translation part:</strong> This uses the translation part of $E_{i,j}$:</p>
<p>\[ RPE_{i,j} = || \mathrm{trans}(E_{i,j}) ||_2 \]</p>
<p><strong>Rotation part:</strong> This uses the rotation part of $E_{i,j}$:</p>
<p>\[ RPE_{i,j} = || \mathrm{rot}(E_{i,j}) - I_{3\times 3}||_{F} \]</p>
<p>While it is usually suggested to compute the RPE for all pairs of timestamps i and j, in this case, we only ask you to compute the error for <strong>Consecutive Frames ONLY</strong>.</p>
<p>Then, different statistics can be calculated on the RPEs, e.g. the RMSE:</p>
<p>\[ \mathrm{RMSE} = \sqrt{\frac{1}{N}\sum_{(i,j)} RPE_{i,j}^2} \]</p>
<p>Compute the RMSE for the given rosbag.</p>
<h3 id="3-publish-your-relative-pose-estimate"> <a href="#3-publish-your-relative-pose-estimate" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 3. Publish your relative pose estimate</h3>
<p>Publish pose estimates with respect to the previous ground-truth pose in order to assess (visually using Rviz) the quality of your pose estimate compared to the real one. To do so, you can use the ground-truth pose of the previous frame (obtained from ROS messages), plus the relative pose between current frame and previous frame (obtained from your algorithms), to compute the estimated (absolute) pose of the current frame, and then publish it.</p>
<p>To run your code, use:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>roslaunch lab_6 video_tracking.launch
</code></pre></div></div>
<p>but be sure to modify the dataset path and parameters to run the correct method! For example, the <code class="highlighter-rouge">pose_estimator</code> parameter determines which algorithm to be used for the motion estimation, and the <code class="highlighter-rouge">use_ransac</code> parameter decides if RANSAC should be used.</p>
<h2 id="-deliverable-5---3d-3d-correspondences-20-pts"> <a href="#-deliverable-5---3d-3d-correspondences-20-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 Deliverable 5 - 3D-3D Correspondences [20 pts]</h2>
<p>The rosbag we provide you also contains depth values registered with the RGB camera, this means that each pixel location in the RGB camera has an associated depth value in the Depth image. Note that not every pixel has a valid depth value.</p>
<p>Keeping this in mind, try to build a point cloud for a given frame using the depth image (which is registered to the RGB camera, and therefore has the same parameters than the RGB camera). For this part, you need to calculate the 3D point cloud for two consecutive frames (this is essentially done for you in the template code) and then use Arun’s algorithm to compute the drone’s relative pose from frame to frame.</p>
<h3 id="1-cameracallback-implement-aruns-algorithm"> <a href="#1-cameracallback-implement-aruns-algorithm" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 1. <code class="highlighter-rouge">cameraCallback</code>: Implement Arun’s algorithm</h3>
<p>Implement <a href="http://laurentkneip.github.io/opengv/namespaceopengv_1_1point__cloud.html#a047c3c5a395a740e7f3f2b8573289211" target="_blank" rel="noopener noreferrer">Arun’s algorithm</a> in this function. Use the evaluateRPE function you used previously to <strong>plot the rotation error and the translation error over time</strong> as well. Mind that, in this case, there is no scale ambiguity, therefore we cannot really compare this approach against the previous ones. Implement Arun’s algorithm both <em>with</em> and <em>without</em> RANSAC.</p>
<p>To run your code, use:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>roslaunch lab_6 video_tracking.launch
</code></pre></div></div>
<p>with the <code class="highlighter-rouge">pose_estimator</code> parameter set to <code class="highlighter-rouge">3</code> so that Arun’s method is used.</p>
<h2 id="summary-of-team-deliverables"> <a href="#summary-of-team-deliverables" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Summary of Team Deliverables</h2>
<p>For the default dataset, we require you to run <strong>all algorithms</strong> on it and compare there performances. While for the Lissajous data, only <strong>Arun’s 3 pt method (with and without RANSAC)</strong> is required. Therefore, as a summary for Team Deliverables:</p>
<ol>
<li>Plots of translation and rotation error for each of the methods (5pt, 8pt with and without RANSAC, 2 pt without RANSAC, Arun 3 pt with and without RANSAC) using the default rosbag.</li>
<li>Plots of translation and rotation error for Arun’s 3 pt method (with and without RANSAC) on the provided Lissajous dataset.</li>
</ol>
<h2 id="-optional-deliverable-6---3d-2d-correspondences-20-pts"> <a href="#-optional-deliverable-6---3d-2d-correspondences-20-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> 📨 [Optional] Deliverable 6 - 3D-2D Correspondences [20 pts]</h2>
<p>Last, but not least, it is possible to compute an Absolute Pose Estimate, instead of the relative one from frame to frame, by using 3D to 2D correspondences. The pose estimation from 3D-2D Correspondences is as follows: you are given a set of 3D points in the world frame (3D coordinates), if we are capable of identifying the corresponding projections in the image (as 2D pixels), then the goal is to compute the absolute pose of the camera in the world frame.</p>
<p>To do so, we make use of the <strong>Perspective-n-Points algorithms</strong> (PnP for short), for which more information can be found <a href="https://en.wikipedia.org/wiki/Perspective-n-Point" target="_blank" rel="noopener noreferrer">here</a>. In particular, one may use Lepetit’s Epnp algorithm which uses all correspondences available. There are other PnP algorithms with different characteristics, OpenGV provides implementations for these as well.</p>
<p>For this exercise, we will use EPnP by using <strong>OpenCV’s implementation</strong>.</p>
<ul>
<li>This is because we will be using a checkerboard to know the precise 3D coordinates of the corners we extract.</li>
<li>To do so, we provide a rosbag with a checkerboard in-sight (<a href="https://www.dropbox.com/s/ocm8gimeqm19ljo/run.bag?dl=0" target="_blank" rel="noopener noreferrer">here</a>).</li>
<li>Your job is to detect the checkerboard corners by using OpenCV’s function <a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findchessboardcorners" target="_blank" rel="noopener noreferrer">findChessboardCorners</a>. This function will return the pixel positions of the corners of the checkerboard, we need also to compute the 3D positions of these.</li>
<li>Calculate the 3D coordinates of each corner inside the checkerboard (not the ones on the border! A total of 9 x 6 corners) knowing that the side of <strong>one square is 0.0254m</strong>. Use as world frame of reference the bottom left corner, as drawn below. Note that the checkerboard is of <strong>size 10 x 7</strong>. The actual position of the world frame of reference does not matter. Nevertheless, the actual description of the 3D points of the corners in this frame of reference is crucial. To make things easy, you should set the Z axis perpendicular to the checkerboard and align the X and Y axis with the actual chessboard squares. Using this, the top left corner of the image below is (6*square_size = 0.1016). <img src="https://mit-spark.github.io/VNAV2020-handouts/assets/images/lab6/image3.png" alt="Checkerboard" class="mx-auto d-block">
</li>
<li>Successfully matching 2D pixels to 3D corners is a matter of keeping a proper ordering between 3D points and the way the 2D corners are extracted. Make sure you read OpenCV’s API on the order that <a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findchessboardcorners" target="_blank" rel="noopener noreferrer">findChessboardCorners</a>. extracts corners, and follow the same order when building the 3D coordinates.</li>
<li>Using these 3D coordinates and the computed 2D corners, use <a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnpransac" target="_blank" rel="noopener noreferrer">OpenCV’s Epnp algorithm</a> to compute the pose of the camera with respect to the frame of reference on the checkerboard.</li>
<li>Create a ROS TF Broadcaster from the <code class="highlighter-rouge">chessboard</code> frame of reference to the drone’s frame of reference (call it <code class="highlighter-rouge">camera</code>).</li>
</ul>
<p>To run your code, use:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>roslaunch lab_6 chessboard.launch
</code></pre></div></div>
<p>but be sure to modify the dataset path in the launch file to point to your bag files!</p>
<hr>
<footer role="contentinfo"><p class="text-small text-grey-dk-000 mb-0"><a href="http://accessibility.mit.edu" target="_blank" rel="noopener noreferrer">Accessibility.</a> Copyright © 2017-2020 MIT SPARK Lab. Using <a href="https://github.com/pmarsceill/just-the-docs/tree/master" target="_blank" rel="noopener noreferrer">Just the docs</a> theme.</p></footer>
</div>
</div>
</div></div>
</div>