{
  "1": {
    "title": "C++",
    "content": "C++ . . How C++ works | Hello world | Standard vector | Classes | Eigen | Where to go now | How C++ works . Warning . This section should hopefully just serve as a refresher to you. If you are completely new to C++ / C, please talk to us so we can direct you to more resources. . C++ is a high level programming language. A computer’s CPU is incapable of speaking C++. The limited set of instructions that a CPU can understand directly is called machine code. . Programs written in high level languages must be translated into machine code before they can be run. There are two primary ways this is done: compiling and interpreting. C++ is a compiled language, meaning that it has to be compiled. . . Hello world . It’s not a programming language if doesn’t allow to print “Hello World”. The very first C++ program that we will look at is one that prints this simple text to the terminal. . Create a new file called hello.cpp (try to use the terminal for this), and write the following code: . #include &lt;iostream&gt; int main(){ std::cout &lt;&lt; &quot;Hello World from VNAV 2020! n&quot;; return 0; } . Let’s have a closer look to the code. On the first line, we include the iostream header file. Header files are mechanisms in C++ through which definitions of functions and classes are exposed to different programs. The first character is the # symbol, which is a marks the statement to the preprocessor. Each time you start your compiler, the preprocessor reads through the source code, looking for lines that begin with the pound symbol (#), and acts on those lines before the compiler runs. The angle brackets around the filename tell the preprocessor to look in all the usual places for this file. If your compiler is set up correctly, the angle brackets will cause the preprocessor to look for the file iostream.h in the directory that holds all the header files in your system. Note that the header file name does not have any extension, this is true for all of the Standard Library header files. The header iostream (Input-Output-Stream) is used by std::cout, which assists with writing to the screen (console output). . Line 3 begins the actual program with a function named int main(). Every C++ program has a main() function, it in facts marks the entry point of the program. When the program starts, main() is called by the operating system. Like all functions, the main must state what kind of value it will return. The return value type for main() in int, which means that this function will a status code. All ANSI-compliant programs declare main() to return an int. This value is “returned” to the operating system when your program completes. Some programmers signal an error by returning the value 1 and success with 0. . All blocks begin with an opening brace ({) and end with a closing brace (}). The main part of our program happen on line 4. The object std::cout is used to print a message to the standard output (i.e. the terminal), you will use the std::cout a lot. Here’s how std::cout is used: type the word std::cout, followed by the output redirection operator (&lt;&lt;). Whatever follows the output redirection operator is written to the screen. If you want a string of characters written, be sure to enclose them in double quotes (&quot;). . The final two characters, n, tell cout to put a new line after the words Hello World! This special code is explained in detail when cout is discussed on Day 17. . Keep in mind. . Around the web you will commonly find using namespace std; to avoid std::. Setting the global namespace is a pretty bad practice, try to avoid it. . To run this program, we must first compile it using a C++. We will be using the C++ compiler in the GNU Compiler Collection. In your terminal run . g++ -std=c++11 -Wall -pedantic -o hello hello.cpp . we are saying to the compiler to enable the C++11 standard (-std=c++11), report all warnings (-Wall) and to strictly adhere to the standard (-pedantic). The output file will be called hello. . If everything works out fine (and it should) you can run your first C++ application! . $ ./hello Hello World from VNAV 2020! . Standard vector . Let’s move to something more, the standard vector. Vector is a template class that implements a variable length array. It allows the same natural syntax that is used with plain arrays but offers a series of services that free the C++ programmer from taking care of the allocated memory and help operating consistently on the contained objects. . The first step using vector is to include the appropriate header: . #include &lt;vector&gt; . There are multiple ways to initialize a vector, the simplest is the following: . std::vector&lt;T&gt; vect; . This widely used notation, T stands for any data type, built-in, or user-defined class. The vector will store the Ts in a contiguous memory area that it will handle for us, and let us access the individual T simply by writing vect[0], vect[1], and so on, exactly like we would do for a C-style array. . Another very useful way to initialize it is the following . std::vector&lt;T&gt; histo(int size, T initial_value); . In this case we are saying to create a vector of size-elements, each initialized with the value initial_value. . Keep in mind. . You can access the vector elements with the square brackets but this will not check if the index is out of bound, it is better to use the at method, e.g. v.at(0) since it will throw an exception if the index is out of bound. . If we do not pre-initialize the vector we must add elements one by one. To add at the end of the vector we can use the push_back(T elem) method. For example we can declare a vector of integers and fill it with 5 random number in the interval 0-99: . // Create a vector containing integers std::vector&lt;int&gt; vec; // Fill the vector with random number in [0, 99] for (auto i = 1; i &lt;= 5; ++i){ vec.push_back(rand() % 100); } . The current size of the vector can be retrieved with the method size() . std::cout &lt;&lt; &quot;Vector size: &quot; &lt;&lt; vec.size() &lt;&lt; std::endl; . The information can be used to access all the elements of the vector: . for(int i = 1; i &lt; vec.size(); ++i){ std::cout &lt;&lt; vec.at(i) &lt;&lt; &quot; &quot;; } . We could have done it in a slight better way (exploiting C++11 feature) using the ranged-based for loops: . for (auto &amp;elem: vec){ std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; } . Note two important things: . We used the keyword auto. For variables, specifies that the type of the variable that is being declared will be automatically deduced. Use it whenever it is clear to understand the type since it allows several optimization from the compiler. | We used the operator &amp; before the name of the variable. This marks the variable as reference to the original value and therefore avoids the copy of the element. | The complete code that declare, initialize and print a vector is listed below . #include &lt;cstdlib&gt; // required for rand #include &lt;iostream&gt; #include &lt;vector&gt; int main(){ // initialize rand with constant seed for reproducibility std::srand(314159); // Create a vector containing integers std::vector&lt;int&gt; vec; // Fill the vector with random number in [0, 99] for (auto i = 1; i &lt;= 5; ++i) vec.push_back(rand() % 100); // Print number of elements in the vector to screen std::cout &lt;&lt; &quot;Vector size: &quot; &lt;&lt; vec.size() &lt;&lt; std::endl; // Print all elements to screen std::cout &lt;&lt; &quot;Vector elements: &quot;; for (auto &amp;elem: vec) std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; std::cout &lt;&lt; std::endl; return 0; } . If you compile and run this code you will see something like . $ ./ex1 Vector size: 5 Vector elements: 19 10 47 39 39 . Classes . C++ is an object-oriented programming language. The main entity of objects are classes. You can think of a class like it was a new data type (like int or double) containing several variables (of different types, called members) and some functions (called methods). . For example you can think to a new type called Circle. A circle has a radius (an intrinsic property), and several functions apply to a circle like computing the circumference and the area. . Declaring a class is very easy . class Circle{}; . With the simple statement we create a class named Circle. Being a new data type we can instantiate a class in the same way we instantiate an integer . Circle c; . The class so far is not very useful, it does not contain any members or methods. To add the member radius we need to change the declaration of the class . class Circle{ double radius; } . Now we have a member but if you try to modify the value you will discover that you cannot: this happens because by default members are declared private, so nobody expect the class methods can change its value. To set the value we need a class constructor. The class constructor is simply a method (without return value) with the same name of the class . class Circle{ double radius; public: Circle(double rad){ radius = rad; } } . As you see now we declared the constructor under the public keyword. It is called access specifier and says who can access members and methods. . Moreover. . We can also declare a member to be public, and in this case it would have been a good idea. In strict object-oriented programming this is usually avoided because if we allow the user to directly change the class properties we lose the opportunity to perform any book-keeping on the class itself (e.g. maybe we have to update other properties when we change the radius of the circle). . Using the same approach of the constructor we can add two more methods: double circumference() and double area() . #include &lt;cmath&gt; // required for pow and M_PI #include &lt;iostream&gt; class Circle{ double radius; public: Circle(double r) { radius = r; } double circumference(){ return 2*M_PI*radius; } double area(){ return M_PI*std::pow(radius,2); } }; int main(){ Circle circ(3); std::cout &lt;&lt; &quot;Circumference: &quot; &lt;&lt; circ.circumference() &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Area: &quot; &lt;&lt; circ.area() &lt;&lt; std::endl; return 0; } . If you compile and run the code above, you should see something like: . $ ./ex2 Circumference: 18.8496 Area: 28.2743 . Eigen . Now that we’ve covered some of the basics syntax of C++, it’s time to take a look at some of the existing libraries out there that will help us throughout this course. Eigen, a fast header-only linear algebra library, is the one we will try out right now. To install Eigen, it is very simple: . sudo apt install libeigen3-dev . What this command does is that it will download all relevant Eigen header files to a folder that will be searched by compilers. In the case of Ubuntu, Eigen’s header files will be downloaded to /usr/include/eigen3 or /usr/local/include/eigen3. . Here’s an example from Eigen’s documentation that will get you started: . #include &lt;iostream&gt; #include &lt;Eigen/Dense&gt; int main() { Eigen::MatrixXd m(2,2); m(0,0) = 3; m(1,0) = 2.5; m(0,1) = -1; m(1,1) = m(1,0) + m(0,1); std::cout &lt;&lt; m &lt;&lt; std::endl; } . Let’s take a look at this example line-by-line. First, we include the &lt;iostream&gt; header for printing to our terminal, which should be familiar to you now. Next, we include the &lt;Eigen/Dense&gt; header, which defines member functions and types that work with dense matrices. &lt;Eigen/Dense&gt; is one of the many modules Eigen provides. If you are interested in learning more, please check out this page. . In the main function, we first declare a variable called m of type Eigen::MatrixXd. The Eigen:: part represents the namespace in which Eigen’s functions and types live. MatrixXd represents a matrix of arbitrary size (notice the X in MaxtriXd), and each entry in the matrix is of type double (hence the d in MatrixXd). We use MatrixXd’s constructor to set the matrix to be 2-by-2. We then use the () operator to set all entries. As usual in computer science, the index of the first entry is 0, instead of 1. The final line in the main function prints the matrix to the standard output stream. . To compile it, assuming it’s saved in a file named eigen-example.cpp, you simply need to run: . g++ -o eigen-example eigen-example.cpp . You should see the following after executing the code above: . $ ./eigen-example 3 -1 2.5 1.5 . Where to go now . C++ is a complex language with many interesting features, we encurage you take a look at the following resources if you want to dive deeper: . Modern C++ Course (Cyrill Stachniss) | Effective Modern C++ | C++ Reference | .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/cpp",
    "relUrl": "/lab1/cpp"
  },
  "2": {
    "title": "Exercises",
    "content": "Exercises . Submission Find a team | | Setup workspace Create the catkin workspace | Getting the Lab code | Building the code | A two-drone scenario The static scenario and rViz | | | Problem formulation Positions | Orientations | | Basic ROS commands 📨 Deliverable 1 - Nodes, topics, launch files (10 pts) | | Let’s make things move! Publishing the transforms using tf Some Context | 📨 Deliverable 2 - Publishing transforms (30 pts) How to test | What to expect | | Changing the rViz fixed reference frame. | 📨 Deliverable 3 - Looking up a transform (30 pts) What to expect | | | Let’s do some math 📨 Deliverable 4 - Mathematical derivations (25 pts) | 📨 Deliverable 5 - More properties of quaternions (5 pts) | 📨 [Optional] Deliverable 6 - Intrinsic vs Extrinsic rotations (20 pts) | | Submission . To submit your solutions create a folder called lab2 and push one or more file to your repository with your answers (it can be plain text, markdown, pdf or whatever other format is reasonably easy to read) . for math-related deliverables, LaTeX is preferred but handwritten is accepted too | for code-based deliverables, push the source code of the entire package | . Deadline: the VNAV staff will clone your repository on Wednesday 16th at midnight EDT. . Find a team . Next lab will have a team section, you should start thinking about your team! . Find a team. . Please find your team members, fill out this spreadsheet. . Please fill it before Wednesday 16th. . Setup workspace . Create the catkin workspace . To get started, we must create a ROS workspace for the VNAV class. Choose your favorite working directory, we will assume you are creating one in your home directory (i.e ~/). In a terminal, run: . $ mkdir -p ~/vnav_ws/src $ cd ~/vnav_ws/ $ catkin init Initializing catkin workspace in `/home/antonap/vnav_ws`. -- Profile: default Extending: [env] /opt/ros/melodic Workspace: /home/antonap/vnav_ws -- Build Space: [missing] /home/antonap/vnav_ws/build Devel Space: [missing] /home/antonap/vnav_ws/devel Install Space: [unused] /home/antonap/vnav_ws/install Log Space: [missing] /home/antonap/vnav_ws/logs Source Space: [exists] /home/antonap/vnav_ws/src DESTDIR: [unused] None -- Devel Space Layout: linked Install Space Layout: None -- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False -- Whitelisted Packages: None Blacklisted Packages: None -- Workspace configuration appears valid. -- . Getting the Lab code . Go the folder where you cloned the Labs codebase and run git pull. This command will update the folder with the latest code. Let’s suppose we have the codebase in ~/labs. In ~/labs/lab2 you now have the two_drones_pkg folder, which is a ROS package. Copy this folder in your VNAV workspace and build the workspace as follows: . cp -r ~/labs/lab2/two_drones_pkg ~/vnav_ws/src . Building the code . Building the code is as easy as running: . catkin build . Now that you built the code you see that catkin added a bunch of new folders. In order to use our workspace, we need to make ROS aware of all the components by sourcing the corresponding environment. This is done by running the following in every single terminal where you intend to use the workspace: . source devel/setup.bash . For the rest of the assignment, we assume that you are performing this operation whenever necessary. . A two-drone scenario . In this part, we are going to work with 3D Rigid Transformations and with tf, a basic tool provided by ROS to keep track of multiple coordinate frames over time. . The static scenario and rViz . To get started, let’s go back to the VNAV workspace and bring up the two-drones static scenario. In this environment, we have two aerial vehicles, AV1 [blue] and AV2 [red] that are not moving, but it serves as a good starting point! With the VNAV workspace sourced in a terminal, run: . roslaunch two_drones_pkg two_drones.launch static:=True . You should see the following window, which shows the initial positions of the two AVs. . . This window is rViz, the mighty ROS visualizer! Just like most processes in the ROS ecosystem, rViz is a ROS node. Like all other nodes, rViz can subscribe to ROS topics. However, its specialty is converting them into graphics on your screen to display the state of your robotic system. . As a first experience with rViz, let us: . Add the visualization of tf. In the Displays panel, click Add, select the By display type Tab in the pop-up and finally select “TF” and confirm with Ok. You should see all the reference frames, with names and their axes represented in red (x), green (y) and blue (z). | Save the configuration file. So that we don’t have to repeat the step above every time we launch it! Hit CTRL + s or select File &gt; Save Config. | . Other published topics can be added to the visualizer in a similar way. . Problem formulation . We consider the scenario illustrated in the picture below, where two aerial vehicles, AV1 [blue] and AV2 [red] are following different trajectories: a circle and an arc of parabola, respectively. In the scene, we have highlighted the following elements: . The world frame $(x_w, y_w, z_w)$ | The AV1 body frame, centered in $O_1$ with axes $(x_1,y_1,z_1)$ | The origin of AV2, denoted with $O_2$ | . . Positions . In the world frame, AV1 and AV2’s origins are given by: . $o_1^w = [ text{cos}(t), text{sin}(t), 0]^T$, and | $o_2^w = [ text{sin}(t), 0, text{cos}(2t)]^T$, | . where $t$ denotes time. . Orientations . We will make the following simplifying assumptions: . AV1’s reference frame is such that $y_1$ stays tangent to AV1’s trajectory for all $t$ and $z_1$ is parallel to $z_w$ for all $t$ (i.e., equivalently, roll = pitch = 0, yaw = $t$) | AV2’s reference frame moves with pure translation and we can assume that its axes are parallel to the world axes for all times $t$ | . Notes . Given the dynamics of a quadrotor, these motions are dynamically infeasible. However, for the purpose of this lab, we disregard this fact and focus on the study of rigid transformations | To make the math of this problem more interesting, we chose the $y_1$ axis to point in the direction of motion of the drone. However, do not forget that the standard convention is that $x_1$ should point forward! | . In the sequel, we reproduce the above scenario in ROS and study the trajectory of AV2 relative to AV1’s coordinate frame. . Basic ROS commands . 📨 Deliverable 1 - Nodes, topics, launch files (10 pts) . With the roslaunch command above we have spawned a number of ROS nodes at the same time. Using your knowledge of ROS, answer the following questions: . List the nodes running in the two-drone static scenario. Hint: you can directly inspect the launch file or get some help from rqt_graph. You will notice that rViz is initially not shown in it but you can uncheck the Debug option for a full picture, feel free to ignore all /rosout nodes and topics and /rqt_gui_py_* that may appear | . | How could you run the two-drone static scenario without using the roslaunch command? List the commands that you would have to execute, in separate terminals, to achieve the same result. Hint: rosrun [...], try things out before finalizing your answer! | . | List the topics that each node publishes / subscribes to. What nodes are responsible for publishing the av1, av2, frames? Which topic causes rViz to plot the drone meshes? Hint: uncheck items on the left pane in rViz until the meshes disappear, then check what node is publishing the corresponding topic | . | What changes if we omit static:=True? Why? Hint: check out and briefly explain the if and unless keywords in the launch file | . | Let’s make things move! Publishing the transforms using tf . After exploring the static scenario, it’s time to implement the motions described in the problem formulation section and visualize them in rViz. With the editor of your choice, open frames_publisher_node.cpp in the src folder of two_drones_pkg. In this file, we provide a basic structure of a ROS node. . Some Context . Please take your time to familiarize with this code before modifying it. Although not mandatory, the pattern found in it is a very common way to implement ROS nodes: . The node’s source code is encapsulated into a class, FramesPublisherNode (line 6), which keeps a nodeHandle as a private member. | In the constructor of the class (lines 14 to 19), one performs operations that need to be executed only once upon startup (e.g. typically, initializing subscribers and publishers), | Using a Timer (lines 10 and 17), the onPublish() method is called periodically - at a 50Hz - and all the operations within it are performed ad libitum, | In the body of main() (towards the end of the file): the node is registered with ros::init(...) | an instance of the node class is created | a blocking call to ros::spin() is issued, which starts ROS’s main loop. | . | . 📨 Deliverable 2 - Publishing transforms (30 pts) . In frames_publisher_node.cpp, follow the instructions in the comments and fill in the missing code. Your objective is to populate the provided AV1World and AV2World variables to match the motions described in the problem formulation. These objects are instances of the tf::transform class, which is ROS jargon for a homogeneous transformation matrix. . Keep in mind. . Ensure that the orientation of the AV1 frame is in accordance with the assumptions made in the problem formulation, as this is of crucial importance for the final result! . How to test . Once you are ready to compile your code, run: . catkin build . from the workspace folder ~/vnav_ws. . To try out your code, launch the two-drone scenario in non-static mode, i.e. run: . roslaunch two_drones_pkg two_drones.launch . What to expect . You should finally see the drones moving! Check that the trajectories reflect those illustrated in the figure in the problem formulation. . . Changing the rViz fixed reference frame. . As mentioned, we are interested in the motion of AV2 relative to AV1’s reference frame. In the Displays panel (left, by default), under the Global Options section, rViz offers the possibility to change the Fixed Frame to any frame published in tf. Try it out yourself and change “world” into “av1” by typing in the corresponding field. From this perspective, AV1 appears static, the world frame spins around its $z$ axis and AV2 seems to be following a closed-curve trajectory. . 📨 Deliverable 3 - Looking up a transform (30 pts) . In plots_publisher_node.cpp, follow the instructions in the comments and fill in the missing code. Your objective is to populate the provided object, transform, with the relative transform between two given frames with names ref_frame and dest_frame. . Compile your code and try it out as previously explained. . What to expect . You should eventually see three trajectories, namely: . AV1’s trajectory [blue, solid] in the world frame (circle on the x-y plane) | AV2’s trajectory [red, solid] in the world frame (parabola on the z-x plane) | The trajectory of AV2 in AV1’s frame [red, dashed]. You should now have a strong hunch that this curve is a ellipse on a “slanted” plane! | . Note: if the results you are observing do not seem to make sense, try swapping ref_frame and dest_frame when interrogating tf. . Let’s do some math . So far, we have used ROS and tf to get a visual understanding of the motion of AV2 relative to AV1’s body frame. In this section, you are asked to use your knowledge about homogeneous transformations and study the relative trajectory explicitly. . The visualization we have built should provide you with great guidance while working out the following questions. Since this exercise is designed for you to familiarize with the math of 3D transformations, we require that you explicitly write down all the homogeneous transformation matrices used in the process and precisely outline the logic and algebraic steps taken. . 📨 Deliverable 4 - Mathematical derivations (25 pts) . In the problem formulation, we mentioned that AV2’s trajectory is an arc of parabola in the $x$-$z$ plane of the world frame. Can you prove this statement? Hint: $cos(2t)$ can be written as… | . | Compute $o_2^1(t)$, i.e., the position of AV2 relative to AV1’s body frame as a function of $t$. Hint: write down the homogeneous transformations and compose them accordingly… | . | Show that $o_2^1(t)$ describes a planar curve and find the equation of its plane $ Pi$. Hint: find a linear relation between $z_2^1$ and $y_2^1$ | . | Rewrite the above trajectory explicitly using a 2D frame of reference $(x_p, y_p)$ on the plane found before. Try to ensure that the curve is centered at the origin of this 2D frame and that $x_p$, $y_p$ are axes of symmetry for the curve. Hints: | .  i) center the new 2D frame in $p^1= (-1,-1/2,0)$, these coordinates are in AV1’s frame |  ii) start with a 3D reference frame centered in p with axes $(x_p, y_p, z_p)$, compute $o_2^p(t)$ |  iii) make sure that the $z$ component vanishes after the change of coordinates | | Using the expression of $o_2^p(t)$, prove that the trajectory of AV2 relative to AV1 is an ellipse and compute the lengths of its semi-axes. Hint: what is the general form of the equation of an axis-aligned ellipse centered in the origin? | . | 📨 Deliverable 5 - More properties of quaternions (5 pts) . In the lecture notes, we have defined two linear maps $ Omega_1: mathbb{R}^4 rightarrow mathbb{R}^{4 times 4}$, and $ Omega_2: mathbb{R}^4 rightarrow mathbb{R}^{4 times 4}$, such that for any $q in mathbb{R}^4$, we have: [ Omega_1(q) = begin{bmatrix} q_4 &amp; -q_3 &amp; q_2 &amp; q_1 q_3 &amp; q_4 &amp; -q_1 &amp; q_2 -q_2 &amp; q_1 &amp; q_4 &amp; q_3 -q_1 &amp; -q_2 &amp; -q_3 &amp; q_4 end{bmatrix}, Omega_2(q) = begin{bmatrix} q_4 &amp; q_3 &amp; -q_2 &amp; q_1 -q_3 &amp; q_4 &amp; q_1 &amp; q_2 q_2 &amp; -q_1 &amp; q_4 &amp; q_3 -q_1 &amp; -q_2 &amp; -q_3 &amp; q_4 end{bmatrix}. ] The product between any two unit quaternions can then be explicitly computed as: [ q_a otimes q_b = Omega_1(q_a) q_b = Omega_2(q_b)q_a. ] In fact, the two linear maps $ Omega_1$ and $ Omega_2$ have more interesting properties, and you are asked to prove the following equalities: . For any unit quaternion $q$, both $ Omega_1(q)$ and $ Omega_2(q)$ are orthogonal matrices, i.e., [ Omega_1(q)^T Omega_1(q) = Omega_1(q) Omega_1(q)^T = I_4, ] [ Omega_2(q)^T Omega_2(q) = Omega_2(q) Omega_2(q)^T = I_4. ] Intuitively, what is the reason that both $ Omega_1(q)$ and $ Omega_2(q)$ must be orthogonal? Hint: what does $q_a otimes q_b$ still being a unit quaternion imply? | . | For any unit quaternion $q$, both $ Omega_1(q)$ and $ Omega_2(q)$ convert $q$ to be the unit quaternion that corresponds to the 3D identity rotation, i.e., [ Omega_1(q)^Tq = Omega_2(q)^Tq = [0,0,0,1]^T. ] . | For any two vectors $x,y in mathbb{R}^4$, show the two linear operators commute, i.e., [ Omega_1(x) Omega_2(y) = Omega_2(y) Omega_1(x), ] [ Omega_1(x) Omega_2(y)^T = Omega_2(y)^T Omega_1(x). ] | 📨 [Optional] Deliverable 6 - Intrinsic vs Extrinsic rotations (20 pts) . Consider the following sequence of rotations: . $R_0$: 90° around $x$ | $R_1$: 180° around $y$ | $R_2$ -30° around $x$ | . A) Extrinsic . The sequence of rotations is applied with respect to a fixed frame of reference (the world frame), as follows: . . Note: the body axes are unlabeled, but represented in red ($x_b$), green ($y_b$), blue ($z_b$) . B) Intrinsic . The sequence of rotations is applied in reverse order with respect to a frame of reference attached to the object (the body frame), as follows: . . Note that the final orientation of the object is the same in both cases! . This property is quite general: it holds regardless of the specific axes and angles of the rotations and for any number of rotations in the sequence. . Could you prove this formally? .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab2/exercises",
    "relUrl": "/lab2/exercises"
  },
  "3": {
    "title": "Exercises",
    "content": "Exercises . Submission Individual | Team | Deadline | | Individual Deliverable 1 - Transformations in Practice (10 pts) | Deliverable 2 - Modelling and control of UAVs (20 pts) | | Team Trajectory tracking for UAVs Getting the codebase | | Launching the TESSE simulator with ROS bridge | Implement the controller | Simulator conventions | Deliverable 3 - Geometric controller for the UAV (50 pts) How to test | Tuning tips | What to expect | | | References | Submission . To submit your solutions create a folder called lab3 and push one or more file to your repository with your answers (it can be plain text, markdown, pdf or whatever other format is reasonably easy to read) . Individual . Please push the deliverables into your personal repository, for math-related questions LaTeX is preferred but handwritten is accepted too. . Team . Create a new repository in VNAV2020-submission called TEAM_&lt;N&gt;, replacing &lt;N&gt; with your team number. For example team 2 will create the repository TEAM_2. . Please push the source code of the entire controller_pkg package in the foder lab3 of the team repository. . Deadline . Deadline: the VNAV staff will clone your repository on September 23th, 2020 at midnight. . Individual . Deliverable 1 - Transformations in Practice (10 pts) . This exercise will help you to understand how to perform transformation between different rotation representations using ROS. In addition, you will gain experience of reading through a documentation in order to find what you need. . A. Message vs. tf . In ROS, there are two commonly used datatypes for quaternions. One is geometry_msgs::Quaternion and the other is tf2::Quaternion. Based on the documentation for tf2 and its package listing page, answer the following questions in a text file called deliverable_1.txt: . Assume we have an incoming geometry_msgs::Quaternion quat_msg that holds the pose of our robot. We need to save it in an already defined tf2::Quaternion quat_tf for further calculations. Write one line of C++ code to accomplish this task. . | Assume we have just estimated our robot’s newest rotation and it’s saved in a variable called quat_tf of type tf2::Quaternion. Write one line of C++ code to convert it to a geometry_msgs::Quaternion type. Use quat_msg as the name of the new variable. . | If you just want to know the scalar value of a tf2::Quaternion, what member function will you use? . | B. Conversion . The following questions will prepare you for converting between different rotation representations in C++ and ROS. . Assume you have a tf2::Quaternion quat_t. How to extract the yaw component of the rotation with just one function call? . | Assume you have a geometry_msgs::Quaternion quat_msg. How to you convert it to an Eigen 3-by-3 matrix? Refer to this and this for possible functions. You probably need two function calls for this. . | Deliverable 2 - Modelling and control of UAVs (20 pts) . A. Structure of quadrotors . . The figure above depicts two quadrotors (a) and (b). Quadrotor (a) is a fully functional UAV, while for Quadrotor (b) someone changed propellers 3 and 4 and reversed their respective rotation directions. . Show mathematically that quadrotor (b) is not able to track a trajectory defined in position $[x,y,z]$ and yaw orientation $ Psi$. . Hint: write down the $F$ matrix (see lecture notes eq. (6.9)) for the two cases (a) and (b) and compare the rank of the two matrices | . B. Control of quadrotors . Assume that empirical data suggest you can approximate the drag force (in the body frame) of a quadrotor body as: . [ F^b = begin{bmatrix} 0.1 &amp; 0 &amp; 0 0 &amp; 0.1 &amp; 0 0 &amp; 0 &amp; 0.2 end{bmatrix} (v^b)^2 ] . With $(v^b)^2= [-v^b_x mid v^b_x mid , , -v^b_y mid v^b_y mid , , -v^b_z mid v^b_z mid]^T $, and $v_x$, $v_y$, $v_z$ being the quadrotor velocities along the axes of the body frame. . With the controller discussed in class (see referenced paper 1), describe how you could use the information above to improve the tracking performance. . Hint: the drag force introduced above is an additional term in the system’s dynamics, which the controller could compensate for explicitly… | . Team . Trajectory tracking for UAVs . In this section, we are going to implement the geometric controller discussed in class on a simulated drone. . Getting the codebase . First, let us install some prerequisites. In a terminal, run: . sudo apt install ros-melodic-mav-msgs ros-melodic-ackermann-msgs . Consistently with the folder structure introduced in previous assignments, we will navigate to our existing clone of the VNAV Labs repository and pull the latest updates: . # change the folder name according to your setup cd ~/labs git pull . In ~/labs/lab_3 we now have the tesse_ros_bridge and the controller_pkg folders, which are both ROS packages. Let us copy these folders in our VNAV workspace and build the workspace as follows: . cp -r ~/labs/lab_3/. ~/vnav_ws/src cd ~/vnav_ws . In your src folder, you should see three folders: catkin_simple, controller_pkg and tesse-ros-bridge. Then, we need install some Python dependencies: . cd ~/vnav_ws/src/tesse-ros-bridge/tesse-interface pip install -r requirements.txt # Dependencies pip install . . At this point, you can both keep your lab 2 code or remove all the folders related to the previous labs. Either way, you need to compile the new code. . catkin build source devel/setup.bash . After doing so, please proceed here to download the binary executable for the simulator we are going to use for lab 3. Unzip the file, put the folder in ~/vnav-builds, and run the following commands: . cd ~/vnav-builds/vnav-2020-lab3/ chmod +x vnav-2020-lab3.x86_64 . This will make sure you can run the executable in the command line. . Launching the TESSE simulator with ROS bridge . In a terminal window, run this command to start the simulator: . cd ~/vnav-builds/vnav-2020-lab3/ ./vnav-2020-lab3.x86_64 . One the simulator starts, you should see the unity simulator (fig. below). . . In the simulator, you can: . press W to turn on all propellers; | press R to respawn the quadrotor at the initial location. | . Note that you can resize the window to help with debugging later on. . Once the workspace has been compiled successfully, with your workspace sourced execute: . # make sure to source devel/setup.bash! roslaunch tesse_ros_bridge tesse_quadrotor_bridge.launch . This launches two ROS nodes: tesse_ros_brige and quadrotor_control_interface. The former receives state messages from our simulator and publishes them in a few ROS topics, and the latter listens to our controller node and sends the propeller commands to our simulator. To visualize the state of our quadrotor, we will use rviz. Run the following command to load our rviz preset: . cd ~/vnav_ws/src/controller_pkg rviz -d rviz/lab3.rviz . You should see a screen similar to this: . . A few highlights: . the purple upward arrow corresponds to the IMU message; | the base_link_gt axes correspond to the body frame of the quadrotor; | the world axes correspond to the world frame in ROS. | . Implement the controller . From the previous section, you must have noticed that the drone is stationary. Unsurprisingly, this happens because the UAV is not receiving any control inputs. In this section, you will be precisely asked to take care of it! . Given four propeller speeds, the simulator propagates forward the UAV dynamics and outputs the current state of the robot. It is your job now to provide the propeller speeds based on the current state of the robot (from the simulator) and a desired one (from a trajectory generator). This problem is extensively described in the geometric controller paper 1, where the authors introduce an almost-globally convergent geometric controller for a quadrotor, also outlined in class. We will ask you to read the paper thoroughly and implement this controller in ROS. . For the purposes of this lab, the trajectory generator is given by the traj_publisher node, which outputs a time-varying desired state along a hard-coded circular trajectory. The navigation stack (almost) implemented in our codebase has the architecture shown below: . . where the ROS nodes are highlighted as vertices in the graph and the ROS topic names are reported on the edges. . In particular: . /desired_state is of type trajectory_msgs/MultiDOFJointTrajectoryPoint and has the following fields: transforms - type geometry_msgs/Transform[]: translation: a 3D vector with the desired position of the c.o.m. in the world frame | rotation: a quaternion rapresenting a rotation (in our case will be only a yaw component) | . | velocities - type geometry_msgs/Twist[]: linear: a 3D vector with the desired velocity of c.o.m. in world frame | angular: we will ignore this field as it is not used by the controller | . | accelerations - type geometry_msgs/Twist[]: linear: a 3d vector with the desired acceleration of c.o.m. in world frame | angular: we will ignore this field as it is not used by the controller | . | . | /current_state is of type nav_msgs/Odometry with the relevant fields: pose.pose - type geometry_msgs/Pose: position: a 3d vector with the current position of the c.o.m. in the world frame | orientation: a quaternion representing the current orientation of the UAV | . | twist.twist - type geometry_msgs/Twist: linear: a 3d vector with the current velocity of c.o.m. in the world frame | angular: a 3d vector with the current angular velocity in the world frame | . | . | /rotor_speed_cmds is of type mav_msgs/Actuators and it contains the relevant field: angular_velocities - type float64[], containing the desired speeds of the propellers. | . | . Simulator conventions . In the picture below, we illustrate the conventions used in unity simulator for the propeller speeds and their positions in the body frame. Please look carefully as this will help you build the correct wrench-to-controls matrix (F2W) in the coding assignment . . Note that . as in most research papers, we assume that, for $ omega_i &gt; 0$, all the propellers produce an upward force (lift). This is consistent with the first row of the matrix in eq. (6.9) on the lecture notes. While this might be confusing at first, note that on real quadrotors the motors are configured to rotate in different directions: two counter-clockwise (CCW), and two clockwise (CW). This configuration is necessary so that our quadrotor will be stable in its yaw direction. There are also two sets of propellers: one set for CCW motors, and another set for CW motors (both will produce upward force if mounted appropriately). See this video for an example. | in the figure, we show the positive spinning directions for the propellers (blue) and the resulting direction of the yaw torques generated by aerodynamic drag. For example: propeller 1’s angular velocity $ omega_1$ is considered positive when propeller 1 spins clockwise (as seen from top), which in turn produces a counter-clockwise drag torque $ tau_{ text{drag}_1}$. The sign of the drag torque is determined by applying right-hand rule: positive for counter-clockwise, and negative for clockwise. | the spinning directions are different from the paper. The expression $ tau_{ text{drag}}^i = (-1)^i c_d , omega_i | omega_i |$ implies that the drag on propeller 1 produces a negative (clockwise) yaw torque instead, for $ omega_1$. However, the above convention is acceptable: propellers are mounted differently in the simulated quadrotor! | the propeller speeds need to be provided in radians per second (rad/s), NOT in rpm. Initially, it will be tempting to go for the latter, as it will make the drone move more easily (control inputs get bigger), however, it will hurt you in the long run. | . Before proceeding, keep in mind that there are a number of caveats that will make the math in your code slightly different from the one found in the paper. The main differences can be summarized as follows: . Reference frames. The paper and your code use different conventions for coordinate frames. Please refer to the figure and pay attention to the two main facts summarized below. . . In the paper, the $z$-axes - both in the world and body frames - are downwards, while ROS has them pointing upwards. This affects the signs of $b_{3_d}$ in equation (12), as well as gravity and aerodynamic forces in equations (12), (15), (16). (equation numbers refer to the paper 1). | In the paper, the $x$-body axis is along a propeller arm, while we prefer having it at 45° between two propellers. This will require some thought on your end when converting the total desired force + torques (wrench) into the desired propeller speeds. In other words, you have to change equation (1) from the paper. For this step, refer to equation (6.9) in the lecture notes! | Aerodynamic coefficients. In the paper, equation (1), the quantity $c_{ tau f}$ relates directly the yaw torque with the lift produced by a propeller. In contrast, we prefer using the lift and drag coefficients $c_f$ and $c_d$, which have a clearer physical interpretation. Consider the relation $c_{ tau f}=c_d/c_f$ between these coefficients. . Deliverable 3 - Geometric controller for the UAV (50 pts) . After reading the reference 1 - thoroughly - get together with your teammates and open the source file controller_node.cpp. In this file, you will find detailed instructions to fill in the missing parts and make your quadrotor fly! To have full credits for this part of the lab, your team needs to complete the following: . Implement all missing parts in controller_node.cpp and ensure everything compiles. | Tune parameters so that the quadrotor achieves stable circular trajectory. | A video showing the quadrotor completing one round of the circular trajectory in rviz (can either be a screen capture, or a video shot using your phone). Please upload the video onto either Google drive or Dropbox, generate a publicly viewable link, and put the link in a text file called rviz_drone_flight.txt in your repo. | . How to test . Once you are ready to compile your code, run: . catkin build . from the folder ~/vnav_ws. . To try out your code, launch the simulator. Additionally, run your controller by typing, in a separate terminal: . roslaunch controller_pkg traj_tracking.launch . Depending on your focus - and degree of luck - it will take multiple attempts to have a stable and smooth controller and multiple iterations of parameter tuning. That’s normal! . As a suggestion, you could start by testing a stationary waypoint, before moving to the circular trajectory. This is possible by switching the STATIC_POSE flag to 1 in controller_pkg/src/traj_publisher.cpp. Also, you should use rviz to visually compare the desired and the current poses. . Tuning tips . First of all, look closely at the equations and see what are the gain parameters directly affecting. Which one affects the yaw? Which one affects the speed? Which one affects the position? Doing so will give you a rough idea on what to do for tuning. . To provide a more graphical/qualitative understanding, here are some typical situations that you might encounter: . komega is too high: . kr is too low: . kv is too low: . NOTE: The specific gain parameters may differ across different computers. . What to expect . If your controller is working reasonably well we expect to see something like this: . . Note: it is totally fine if the drone does a rapid maneuver to start tracking the circle as soon as you run the controller node. . Good luck! . References . Lee, Taeyoung, Melvin Leoky, N. Harris McClamroch. “Geometric tracking control of a quadrotor UAV on SE (3).” Decision and Control (CDC), 49th IEEE Conference on. IEEE, 2010 Link &#8617; &#8617;2 &#8617;3 &#8617;4 . |",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab3/exercises",
    "relUrl": "/lab3/exercises"
  },
  "4": {
    "title": "Exercises",
    "content": "Exercises . Submission Individual | Team | Deadline | | 👤 Individual 📨 Deliverable 1 - Single-segment trajectory optimization (20 pts) | 📨 Deliverable 2 - Multi-segment trajectory optimization (15 pts) | | 👥 Team 📨 Deliverable 3 - Drone Racing (65 pts) Getting the codebase | Journey of a thousand miles starts with a single step | Waypoint publishing | Trajectory generation and following | Ready, go! | [Optional] Faster, faster (Extra credit: 15 pts) | | | Submission . To submit your solutions create a folder called lab4 and push one or more files to your repository with your answers. . Individual . Please push the deliverables into your personal repository, for math-related questions only typeset PDF files are allowed (e.g., using Latex, Word, Markdown). . Team . Deadline . Deadline: the VNAV staff will clone your repository on September 30th at midnight ET. . 👤 Individual . 📨 Deliverable 1 - Single-segment trajectory optimization (20 pts) . Consider the following minimum velocity ($r=1$) single-segment trajectory optimization problem: . begin{eqnarray} min_{P(t)} quad int_0^1 (P^{(1)}(t))^2 dt, label{eq:minvel} s.t. quad P(0) = 0, label{eq:initpos} quad P(1) = 1, label{eq:finalpos} end{eqnarray} . with $P(t) in mathbb{R}[t]$, i.e., $P(t)$ is a polynomial function in $t$ with real coefficients: . begin{equation} P(t) = p_N t^N + p_{N-1} t^{N-1} + dots + p_1 t + p_0. end{equation} . Note that because of constraint ( ref{eq:initpos}), we have $P(0)=p_0=0$, and we can parametrize $P(t)$ without a scalar part $p_0$. . 1. Suppose we restrict $P(t) = p_1 t$ to be a polynomial of degree 1, what is the optimal solution of problem ( ref{eq:minvel})? What is the value of the cost function at the optimal solution? . 2. Suppose now we allow $P(t)$ to have degree 2, i.e., $P(t) = p_2t^2 + p_1 t$. . (a) Write $ int_0^1 (P^{(1)}(t))^2 dt$, the cost function of problem ( ref{eq:minvel}), as $ boldsymbol{p}^T boldsymbol{Q} boldsymbol{p}$, where $ boldsymbol{p} = [p_1,p_2]^T$ and $ boldsymbol{Q} in mathcal{S}^2$ is a symmetric $2 times 2$ matrix. . (b) Write $P(1) = 1$, constraint ( ref{eq:finalpos}), as $ boldsymbol{A} boldsymbol{p} = boldsymbol{b}$, where $ boldsymbol{A} in mathbb{R}^{1 times 2}$ and $ boldsymbol{b} in mathbb{R}$. . (c) Solve the Quadratic Program (QP): begin{equation} min_{ boldsymbol{p}} boldsymbol{p}^T boldsymbol{Q} boldsymbol{p} quad s.t. quad boldsymbol{A} boldsymbol{p} = boldsymbol{b}. label{eq:QPtrajOpt} end{equation} You can solve it by hand, or you can solve it using numerical QP solvers (e.g., you can easily use the quadprog function in Matlab). What is the optimal solution you get for $P(t)$, and what is the value of the cost function at the optimal solution? Are you able to get a lower cost by allowing $P(t)$ to have degree 2? . 3. Now suppose we allow $P(t) = p_3t^3 + p_2 t^2 + p_1 t$: . (a) Let $ boldsymbol{p} = [p_1,p_2,p_3]^T$, write down $ boldsymbol{Q} in mathcal{S}^3, boldsymbol{A} in mathbb{R}^{1 times 3}, boldsymbol{b} in mathbb{R}$ for the QP ( ref{eq:QPtrajOpt}). . Solution: $P^{(1)}(t) = p_1 + p_2 cdot (2t) + p_3 cdot (3t^2)$. Therefore, we have: begin{equation} boldsymbol{Q} = int_0^1 left[ begin{array}{c} 1 2t 3t^2 end{array} right] left[ begin{array}{ccc} 1 &amp; 2t &amp; 3t^2 end{array} right] dt = int_0^1 left[ begin{array}{ccc} 1 &amp; 2t &amp; 3t^2 2t &amp; 4t^2 &amp; 6t^3 3t^2 &amp; 6t^3 &amp; 9t^4 end{array} right] dt = left[ begin{array}{ccc} 1 &amp; 1 &amp; 1 1 &amp; frac{4}{3} &amp; frac{3}{2} 1 &amp; frac{3}{2} &amp; frac{9}{5} end{array} right]. end{equation} In addition, we have: begin{equation} boldsymbol{A} = [1,1,1], boldsymbol{b} = 1. end{equation} . (b) Solve the QP, what optimal solution do you get? Do this example agree with the result we learned from Euler-Lagrange equation in class? . 4. Now suppose we are interested in adding one more constraint to problem ( ref{eq:minvel}): . begin{eqnarray} min_{P(t)} quad int_0^1 (P^{(1)}(t))^2 dt, label{eq:minveladd} s.t. quad P(0) = 0, quad P(1) = 1, quad P^{(1)}(1) = -2. end{eqnarray} Using the QP method above, find the optimal solution and optimal cost of problem ( ref{eq:minveladd}) in the case of: . (a) $P(t) = p_2t^2 + p_1 t$, and . (b) $P(t) = p_3t^3 + p_2 t^2 + p_1t$. . 📨 Deliverable 2 - Multi-segment trajectory optimization (15 pts) . 1. Assume our goal is to compute the minimum snap trajectory ($r= 4$) over $k$ segments. How many and which type of constraints (at the intermediate points and at the start and end of the trajectory) do we need in order to solve this problem? Specify the number of waypoint constraints, free derivative constraints and fixed derivative constraints. . Hint: According to Euler-Lagrange method, what is the degree of the polynomial of each segment? | Hint: How many unknown parameters do we need to solve? | Hint: How many constraints does each waypoint/free derivative/fixed derivative constraint provide? | Hint: See figure for $k=3$ as described in the lecture notes. | . . 2. Can you extend the previous question to the case in which the cost functional minimizes the $r$-th derivative and we have $k$ segments? . 👥 Team . 📨 Deliverable 3 - Drone Racing (65 pts) . For this lab we will be racing our simulated our simulated quadcopters in a drone racing course we prepared in our Unity simulator! . Implement all the missing parts in the code (labeled as Part 0, Part 1.1, Part 1.2, and Part 1.3, see below) | A video showing the quadrotor completing the race course. Please upload the video onto either Google drive or Dropbox, generate a publicly viewable link, and put the link in a text file called rviz_drone_race.txt in your repo. | A rosbag of your complete and fastest run. Upload the rosbag onto either Google drive or Dropbox, generate a publicly viewable link, and put the link also in rviz_drone_race.txt. To record the rosbag, rosbag record /current_state /desired_state . | . Getting the codebase . First, let us install some prerequisites. In a terminal, run: . Next, let’s set up our workspace: . # change the folder name according to your setup cd ~/labs git pull . In ~/labs/lab4 we now have the planner_pkg, trajectory_generation_pkg, and dependencies folders, the first two are ROS packages that you will be modifying for this lab, and dependencies should contain all the dependencies you need for this lab. . cp -r ~/labs/lab4/. ~/vnav_ws/src cd ~/vnav_ws . In your src folder, you should see the folders: controller_pkg, tesse-ros-bridge, planner_pkg, trajectory_generation_pkg, and dependencies. Now, compile the new code. . catkin build source devel/setup.bash . After doing so, please proceed here to download the new binary executable for the simulator we are going to use for lab 4. Unzip the file, put the folder in ~/vnav-builds, and run the following commands: . cd ~/vnav-builds/vnav-2020-lab4/ chmod +x vnav-2020-lab4.x86_64 . Try launch the simulator, the simulator should look like this . Before you start coding, keep in mind that the system we are aiming for is as follows. Pieces are coming together and we are getting closer and closer to the full system that we have seen so often in class! . Journey of a thousand miles starts with a single step . As a warm up exercise, let’s just fly and hover at the first gate! Follow the instructions for Part 0 inside planner_pkg/src/simple_traj_planner.cpp . Now fire up the simulator and test. In one terminal window, run . cd ~/vnav-builds/vnav-2020-lab4/ ./vnav-2020-lab4.x86_64 . In another terminal, launch tesse_ros_bridge to connect ROS to the simulator, . roslaunch tesse_ros_bridge tesse_quadrotor_bridge.launch . Finally launch the test to hover at the first gate . roslaunch planner_pkg static_point_test.launch . Hint: you can press r to respawn your quadcopter | Hint: review the handout from lab 3 if you have trouble running the simulator. | . Waypoint publishing . We have already written this node for you in planner_pkg/src/traj_vertices_publisher.cpp. What we are doing here is reading the position and orientation of the gates in the racing course and publishing them as waypoints for trajectory optimization. The topic to note here is /desired_traj_vertices, which should contain a geometry_msgs::PoseArray type storing the position and orientation of the gates on the racing course. . Trajectory generation and following . Follow the instructions for Part 1 in trajectory_generation_pkg/src/trajectory_generation_node.cpp to get your quadcopter ready for drone racing! This node will subscribe to the waypoints published and use them for trajectory optimization using the mav_trajectory_generation library, and then based on the trajectory, publish the desired state at time t to your controller. . Hint: use vertex.addConstraint(POSITION, position) where position is of type Eigen::Vector3d to enforce a waypoint position. | Hint: use vertex.addConstraint(ORIENTATION, yaw) where yaw is a double to enforce a waypoint yaw. | Hint: remember angle wraps around 2$ pi$. Be careful! | Hint: for the ending waypoint for position use end_vertex.makeStartOrEnd as seen with the starting waypoint instead of vertex.addConstraint as you would do for the other waypoints. | . Ready, go! . Now we are ready to race! Fire up the virtual race course. . cd ~/vnav-builds/vnav-2020-lab4/ ./vnav-2020-lab4.x86_64 . In another terminal, launch tesse_ros_bridge to connect ROS to the simulator, . roslaunch tesse_ros_bridge tesse_quadrotor_bridge.launch . Launch the trajectory follower, . roslaunch trajectory_generation traj_following.launch . And finally, launch the waypoint publisher . roslaunch planner_pkg traj_gen.launch . If everything is working well, your drone should be gracefully going through each gate like this . . Note: If the quadcopter flips over after launching the trajectory follower, press r in the Unity window to respwan, the quadcopter should just go back to the starting position. . [Optional] Faster, faster (Extra credit: 15 pts) . How can you make the drone go faster? We will award extra credit to the fastest 3 teams! .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab4/exercises",
    "relUrl": "/lab4/exercises"
  },
  "5": {
    "title": "Exercises",
    "content": "Exercises . Submission . To submit your solutions you are required to create a repository in the VNAV2020-Submissions organization in github.mit.edu (this will be your first exercise). . VNAV staff will clone your repository from github.mit.edu on September 9 at midnight (11:59 EDT). This will be considered as your submission and will be graded accordingly. . Late Submission. . We will assume your repository is ready to be graded at the deadline, so please email us if you want to submit later than the deadline. . Exercises . Git (5 pts) . In this exercise you are required to set a git repository inside the VNAV2020-Submission organization. This is require for the correct submission of all the exercises of the class. . Create a repository for your personal submissions Go to https://github.mit.edu/organizations/VNAV2020-submissions/repositories/new to create a new repository | Create a new Private repository and call it as your Kerberos username, e.g. if your MIT email is astark@mit.edu, call it astark | Clone the repository to ~/vnav-personal (you will have a team submission later) running git clone git@github.mit.edu:VNAV2020-submissions/YOUR_USERNAME.git ~/vnav-personal (replace YOUR_USERNAME with the name of the repo you just created) | Create a folder called lab1 | . | Clone https://github.mit.edu/VNAV2020/labs in a folder of your choice | You are required to put your solutions in the repository you created in the first Git exercise. . Warning. . If you created the repository in your personal account instead of VNAV2020-submissions you need to transfer the ownership in order to complete your submission. Scroll to the bottom of the page for instructions. . Shell (35 pts) . Exercise 1 - Answer to the following questions Download https://raw.githubusercontent.com/dlang/druntime/master/benchmark/extra-files/dante.txt (try using wget) | Create a file called exercise1.txt in ~/vnav-personal/lab1 and answer to the following questions How many lines does it contains? | How many words does it contains? | How many lines are not blank? | | Push the file to git | . | Exercise 2 - Output redirecting Install fortune-mod using apt | After installation, type fortune in your terminal to see a (hopefully) interesting proverb/quote | Run fortune 5 more times and each time redirect the output to a file called fortunes.txt in ~/vnav-personal/lab1 (Hint: do not recreate the file 5 times - each time a new proverb should be added to the end of fortunes.txt) | Push the file to git | . | Hint: For the first exercise you might want to use the command wc (Word Count). . C++: Warm-up Exercises (20 pts) . Feel free to refer to this when answering the following questions. Some of the questions below are based on C++ Primer, which is also an excellent resource for C++ programming. Put all answers into a text file called cpp-warmup.txt and push it to git. . Operators . What are the values of i and j after running the following code? int i = 0, j; j = ++i; j = i++; . | What does the following code print? int i = 42; std::string output = (i &lt; 42) ? &quot;a&quot; : &quot;b&quot;; std::cout &lt;&lt; output &lt;&lt; std::endl; . | References and Pointers . What does the following code print? int i; int&amp; ri = i; i = 5; ri = 10; std::cout &lt;&lt; i &lt;&lt; &quot; &quot; &lt;&lt; ri &lt;&lt; std::endl; . | What does the following code print? int i = 42; int* j = &amp;i; *j = *j**j; std::cout &lt;&lt; *j &lt;&lt; std::endl; . | What does the following code print? int i[4] = {42,24,42,24}; *(i+2) = *(i+1)-i[3]; std::cout &lt;&lt; *(i+2) &lt;&lt; std::endl; . | What does the following code print? | void reset(int &amp;i) { i = 0; } int j = 42; reset(j); std::cout &lt;&lt; j &lt;&lt; std::endl; . Numbers . What are the differences between int, long, long long, and short? | What are the differences between a float and double? What is the value of i after running the following code snippet? int i; i = 3.14; . | What are the differences between an unsigned and signed type? What is the value of c in the following code snippet assuming chars are 8-bit? unsigned char c = -1; . | What will the value of i be after running the following code snippet? int i = 42; if (i) { i = 0; } else { i = 43; } . | C++: RandomVector (40 pts) . In this exercise we will implement the class RandomVector. Inside ~/vnav-personal/lab1 create a folder called RandomVector and copy the content from https://github.mit.edu/VNAV2020/Labs/tree/master/lab1. . The class RandomVector defined in the header file random_vector.h abstract a vector of doubles. You are required to implement the following methods: . RandomVector(int size, double max_val = 1) (constructor): initialize a vector of doubles of size size with random values between 0 and max_val (default value 1) | double mean() returns the mean of the values in random vector | double max() returns the max of the values in random vector | double min() returns the min of the values in random vector | void print() prints all the values in the random vector | void printHistogram(int bins) computes the histogram of the values using bins number of bins between min() and max() and print the histogram itself (see the example below). | . To to so complete all the TODOs in the file random_vector.cpp. When you are done compile the application by running . g++ -std=c++11 -Wall -pedantic -o random_vector main.cpp random_vector.cpp . Note: we expect you to not use the function from the &lt;algorithm&gt; header. . If you complete correctly the exercise you should see something like . $ ./random_vector 0.458724 0.779985 0.212415 0.0667949 0.622538 0.999018 0.489585 0.460587 0.0795612 0.185496 0.629162 0.328032 0.242169 0.139671 0.453804 0.083038 0.619352 0.454482 0.477426 0.0904966 Mean: 0.393617 Min: 0.0667949 Max: 0.999018 Histogram: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** . Optional (10 pts): Try to implement the methods with and without the functions available in the header &lt;algorithm&gt;. . . Transfer ownership of Git repository . If you created the repository in your personal account instead of VNAV2020-submissions you might want to transfer the ownership in order to complete your submission. . On GitHub, navigate to the main page of the repository. | Under your repository name, click Settings. | Scroll down until your reach the Danger Zone, then click Transfer. | Type the name of your repository in the first row and VNAV2020-submissions in the second, then click I understand, transfer this repository. | Done! |",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/exercises",
    "relUrl": "/lab1/exercises"
  },
  "6": {
    "title": "Exercises",
    "content": "Exercises . Submission Individual | Team | Deadline | | 👤 Individual 📨 Deliverable 1 - Practice with Perspective Projection [10 pts] | 📨 Deliverable 2 - Vanishing Points [10 pts] | | 👥 Team | Update the lab codebase | Feature Tracking and Matching Descriptor-based Feature Matching Feature Detection (SIFT) | | 📨 Deliverable 3 - Feature Descriptors (SIFT) [20 pts] | 📨 Deliverable 4 - Descriptor-based Feature Matching [10 pts] | 📨 Deliverable 5 - Keypoint Matching Quality [10 pts] | 📨 Deliverable 6 - Comparing Feature Matching Algorithms on Real Data [20 pts] 6.a. Pair of frames | 6.b. Real Datasets | | 📨 Deliverable 7 - Feature Tracking: Lucas Kanade Tracker [20 pts] | 📨 [Optional] Deliverable 8 - Optical Flow [+20 pts] | | Submission . To submit your solutions create a folder called lab5 and push one or more file to your repository with your answers. . Individual . Please push the deliverables into your personal repository, for math-related questions only typeset PDF files are allowed (e.g., using Latex, Word, Markdown). . Team . Please push the source code for the entire package to the folder lab5 of the team repository. For the tables and discussion questions, please push a PDF to the lab5 folder of your team repository. . Deadline . Deadline: the VNAV staff will clone your repository on October 7th at 11:59 PM EDT. . 👤 Individual . 📨 Deliverable 1 - Practice with Perspective Projection [10 pts] . Consider a sphere with radius $r$ centered at $[0 0 d]$ with respect to the camera coordinate frame (centered at the optical center and with axis oriented as discussed in class). Assume $d &gt; r + 1$ and assume that the camera has principal point at $(0,0)$, focal length equal to 1, pixel sizes $s_x = s_y = 1$ and zero skew $s_ theta = 0$ (see lecture notes for notation) the following exercises: . Derive the equation describing the projection of the sphere onto the image plane. Hint: Think about what shape you expect the projection on the image plane to be, and then derive a characterisitic equation for that shape in the image plane coordinates $u,v$ along with $r$ and $d$. | . | Discuss what the projection becomes when the center of the sphere is at an arbitrary location, not necessarily along the optical axis. What is the shape of the projection? | 📨 Deliverable 2 - Vanishing Points [10 pts] . Consider two 3D lines that are parallel to each other. As we have seen in the lectures, lines that are parallel in 3D may project to intersecting lines on the image plane. The pixel at which two 3D parallel lines intersect in the image plane is called a vanishing point. Assume a camera with principal point at (0,0), focal length equal to 1, pixel sizes $s_x = s_y = 1$ and zero skew $s_ theta = 0$ (see lecture notes for notation). Complete the following exercises: . Derive the generic expression of the vanishing point corresponding to two parallel 3D lines. | Find (and prove mathematically) a condition under which 3D parallel lines remain parallel in the image plane. | Hint: For both 1. and 2. you may use two different approaches: | Algebraic approach: a 3D line can be written as a set of points $p( lambda) = p_0 + lambda u$ where $p_0 in mathbb{R}^3$ is a point on the line, $u in mathbb{R}^3$ is a unit vector along the direction of the line, and $ lambda in mathbb{R}$. | Geometric approach: the projection of a 3D line can be understood as the intersection beetween two planes. | . 👥 Team . Update the lab codebase . Assuming you have already cloned our Labs repository, you simply need to pull the latest changes! . cd ~/Labs git pull origin master . Copy the contents of the lab5 directory into the src folder of your catkin workspace: . cp -r ~/Labs/lab5 ~/vnav_ws/src cd ~/vnav_ws/src . Make sure to keep the lab3 and lab4 folders in your catkin workspace, too! Some of their dependencies (specifically glog_catkin and catkin_simple) are also required by lab5. Finally, build the code: . cd ~/vnav_ws catkin build -j$(nproc) lab_5 . NOTE: Building the opencv3_catkin package may take some time. As a point of comparison, running with catkin build -j8 on my machine, it took 5.5 minutes. If you run into an error with ‘gflags’ when trying to build opencv3_catkin, run catkin clean to clean your workspace, then build opencv3_catkin before building the rest of the code, i.e.: . catkin build -j$(nproc) opencv3_catkin catkin build -j$(nproc) lab_5 . ATTENTION. . NOTE: There was a bug in lab5/feature_tracking/launch/two_frames_tracking.launch where &quot;/ &gt;&quot; should have been replaced with &quot; /&gt;&quot;. Please pull the updated code or make the change yourself. . Feature Tracking and Matching . Feature tracking between images is a fundamental module in many computer vision applications, as it allows us both to triangulate points in the scene and, at the same time, estimate the position and orientation of the camera from frame to frame (you will learn how to do so in subsequent lectures). . Descriptor-based Feature Matching . Feature Detection (SIFT) . Feature detection consists in extracting keypoints (pixel locations) in an image. In order to track features reliably from frame to frame, we need to specify what is a good feature to detect. . The ideal features in an image are the ones that have the following properties: . Repeatability: the same feature can be found in several images despite geometric (i.e. rotations, scale changes, etc) and photometric transformations (i.e. changes in brightness). | Saliency: each feature has a distinctive description. Otherwise, matching between features is difficult if all features look the same. | Compactness and efficiency: fewer features than image pixels. | Locality: a feature occupies a relatively small area of the image, making it robust to clutter and occlusion. | . As you have seen in the lecture, one very successful feature detector is SIFT (Scale Invariant Feature Transform), which is not only invariant to image rotations, but also to scale changes of the image. . Let us now use SIFT to detect keypoints in an image to see what is the actual output. . We want you to get familiar with state-of-the-art feature detectors, so we will avoid having you re-implement the detectors themselves and focus instead on their comparison. Refer to the tutorials in OpenCV for the different algorithms we will be using for more details. For example, SIFT is detailed here and its OpenCV API is defined here. You will need to search through the OpenCV documentation for details on the other descriptors and methods mentioned in this handout. . NOTE: Check your OpenCV version. . 📨 Deliverable 3 - Feature Descriptors (SIFT) [20 pts] . We provide you with skeleton code for the base class FeatureTracker that provides an abstraction layer for all feature trackers. Furthermore, we give you two empty structures for the SIFT and SURF methods that derive from the class FeatureTracker. . Inside the lab5 folder, we provide you with two images ‘box.png’ and ‘box_in_scene.png’ (inside the images folder). . We will first ask you to extract keypoints from both images using SIFT. For that, we refer you to the skeleton code in the src folder named track_features.cpp. Follow the instructions written in the comments; specifically, you will need to complete: . The stub SiftFeatureTracker::detectKeypoints() and SiftFeatureTracker::describeKeypoints() in lab5/feature_tracking/src/sift_feature_tracker.cpp | The first part of FeatureTracker::trackFeatures() in lab5/feature_tracking/src/feature_tracker.cpp | . Once you have implemented SIFT, you can test it by running: . roslaunch lab_5 two_frames_tracking.launch descriptor:=SIFT # note you can change the descriptor later . Your code should be able to plot a figure like the one below (keypoints you detected should not necessarily coincide with the ones in the figure): . . Now that we have detected keypoints in each image, we need to find a way to uniquely identify these to subsequently match features from frame to frame. Feature descriptors in the literature are multiple, and while we will not review all of them, they all rely on a similar principle: using the pixel intensities around the detected keypoint to describe the feature. . Descriptors are multidimensional and are typically represented by an array. . Follow the skeleton code in src to compute the descriptors for all the extracted keypoints. . Hint: In OpenCV, SIFT and the other descriptors we will look at are children of the “Feature2D” class. We provide you with a SIFT detector object, so look at the Feature2D “Public Member Functions” documentation to determine which command you need to detect keypoints and get their descriptors. | . 📨 Deliverable 4 - Descriptor-based Feature Matching [10 pts] . With the pairs of keypoint detections and their respective descriptors, we are ready to start matching keypoints between two images. It is possible to match keypoints just by using a brute force approach. Nevertheless, since descriptors can have high-dimensionality, it is advised to use faster techniques. . In this exercise, we will ask you to use the FLANN (Fast Approximate Nearest Neighbor Search Library), which provides instead a fast implementation for finding the nearest neighbor. This will be useful for the rest of the problem set, when we will use the code in video sequences. . What is the dimension of the SIFT descriptors you computed? | Compute and plot the matches that you found from the box.png image to the box_in_scene.png. | You might notice that naively matching features results in a significant amount of false positives (outliers). There are different techniques to minimize this issue. The one proposed by the authors of SIFT was to calculate the best two matches for a given descriptor and calculate the ratio between their distances: Match1.distance &lt; 0.8 * Match2.distance This ensures that we do not consider descriptors that have been ambiguously matched to multiple descriptors in the target image. Here we used the threshold value that the SIFT authors proposed (0.8). | Compute and plot the matches that you found from the box.png image to the box_in_scene.png after applying the filter that we just described. You should notice a significant reduction of outliers. | Specifically, you will need to complete: . The stub SiftFeatureTracker::matchDescriptors() in feature_tracking/src/sift_feature_tracker.cpp | The second part of FeatureTracker::trackFeatures() in feature_tracking/src/feature_tracker.cpp . | Hint: Note that the matches object is a pointer type, so to use it you will usually have to type *matches. Once you’ve used the FLANN matcher to get the matches, if you want to iterate over all of them you could use a loop like: for (auto&amp; match : *matches) { // check match.size(), match[0].distance, match[1].distance, etc. } Likewise, good_matches is a pointer so to add to it you will need good_matches-&gt;push_back(...). | . 📨 Deliverable 5 - Keypoint Matching Quality [10 pts] . Excellent! Now, that we have the matches between keypoints in both images, we can apply many cool algorithms that you will see in subsequent lectures. . For now, let us just use a blackbox function which, given the keypoints correspondences from image to image, is capable of deciding whether some matches are considered outliers. . Using the function we gave you, compute and plot the inlier and outlier matches, such as in the following figure: Hint: Note that FeatureTracker::inlierMaskComputation computes an inlier mask of type std::vector&lt;uchar&gt;, but for cv::drawMatches you will need a std::vector&lt;char&gt;. You can go from one to the other by using: std::vector&lt;char&gt; char_inlier_mask{inlier_mask.begin(), inlier_mask.end()}; | Hint: You will need to call the cv::drawMatches function twice, first to plot the everything in red (using cv::Scalar(0,0,255) as the color), and then again to plot inliers in green (using the inlier mask and cv::Scalar(0,255,0)). The second time, you will need to use the DrawMatchesFlags::DRAW_OVER_OUTIMG flag to draw on top of the first output. To combine flags for the cv::drawMatches function, use the bitwise-or operator: DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS | DrawMatchesFlags::DRAW_OVER_OUTIMG | . | Now, we can calculate useful statistics to compare feature tracking algorithms. First, let’s compute statistics for SIFT. Submit a table similar to the following for SIFT (you might not get the same results, but they should be fairly similar): | Statistics Approach . | SIFT | ... | . # of Keypoints in Img 1 | 603 | | . # of Keypoints in Img 2 | 969 | | . # of Matches | 603 | | . # of Good Matches | 93 | | . # of Inliers | 78 | | . Inlier Ratio | 83.9% | | . 📨 Deliverable 6 - Comparing Feature Matching Algorithms on Real Data [20 pts] . The most common algorithms for feature matching use different detection, description, and matching techniques. We’ll now try different techniques and see how they compare against one another: . 6.a. Pair of frames . Fill the previous table with results for other feature tracking algorithms. We ask that you complete the table using the following additional algorithms: . SURF (a faster version of SIFT) | ORB (we will use it for SLAM later!) | FAST (detector) + BRIEF (descriptor) | Hint: The SURF functions can be implemented exactly the same as SIFT, while ORB and FAST can also be implemented exactly the same way except that the FLANN matcher must be initialized with a new parameter like this: FlannBasedMatcher matcher(new flann::LshIndexParams(20, 10, 2));. This is not necessarily the best solution for ORB and FAST however, so we encourage you to look into other methods (e.g. the Brute-Force matcher instead of the FLANN matcher) if you have time. | . We have provided method stubs in the corresponding header files for you to implement in the CPP files. Please refer to the OpenCV documentation, tutorials, and C++ API when filling them in. You are encouraged to modify the default parameters used by the features extractors and matchers. A complete answer to this deliverable should include a brief discussion of what you tried and what worked the best. . ATTENTION. . NOTE: If you have the issue &quot;Unknown interpolation method in function &#39;resize&#39;&quot; for your ORB feature tracker, explicitly set the number of levels in the ORB detector to 1 see OpenCV API here) . By now, you should have four algorithms, with their pros and cons, capable of tracking features between frames. . Hint: It is normal for some descriptors to perform worse than others, especially on this pair of images – in fact, some may do very poorly, so don’t worry if you observe this. | . 6.b. Real Datasets . Let us now use an actual video sequence to track features from frame to frame and push these algorithms to their limit! . We have provided you with a set of datasets in rosbag format here. Please download the following datasets, which are the two “easiest” ones: . 30fps_424x240_2018-10-01-18-35-06.bag | vnav-lab5-smooth-trajectory.bag | . Testing other datasets may help you to identify the relative strengths and weaknesses of the descriptors, but this is not required. . We also provide you with a roslaunch file that executes two ROS nodes: . roslaunch lab_5 video_tracking.launch path_to_dataset:=/home/$USER/Downloads/&lt;NAME_OF_DOWNLOADED_FILE&gt;.bag . One node plays the rosbag for the dataset | The other node subscribes to the image stream and is meant to compute the statistics to fill the table below | . You will need to first specify in the launch file the path to your downloaded dataset. Once you’re done, you should get something like this: . Hint: You may need to change your plotting code in FeatureTracker::trackFeatures to call cv::waitKey(10) instead of cv::waitKey(0) after imshow in order to get the video to play continuously instead of frame-by-frame on every keypress. | . . Finally, we ask you to summarize your results in one table for each dataset and asnwer some questions: . Compute the average (over the images in each dataset) of the statistics on the table below for the different datasets and approaches. You are free to use whatever parameters you find result in the largest number of inliers. | . Statistics Approach for Dataset X . | SIFT | SURF | ORB | FAST+BRIEF | . # of Keypoints in Img 1 | ... | ... | ... | ... | . # of Keypoints in Img 2 | ... | ... | ... | ... | . # of Matches | ... | ... | ... | ... | . # of Good Matches | ... | ... | ... | ... | . # of Inliers | ... | ... | ... | ... | . Inlier Ratio | ...% | ...% | ...% | ...% | . What conclusions can you draw about the capabilities of the different approaches? Please make reference to what you have tested and observed. . | Hint: We don’t expect long answers, there aren’t specific answers we are looking for, and you don’t need to answer every suggested question below! We are just looking for a few sentences that point out the main differences you noticed and that are supported by your table/plots/observations.Some example questions to consider: | Which descriptors result in more/fewer keypoints? | How do they the descriptors differ in ratios of good matches and inliers? | Are some feature extractors or matchers faster than others? | What applications are they each best suited for? (e.g. when does speed vs quality matter) | . 📨 Deliverable 7 - Feature Tracking: Lucas Kanade Tracker [20 pts] . So far we have worked with descriptor-based matching approaches. As you have seen, these approaches match features by simply comparing their descriptors. Alternatively, feature tracking methods use the fact that, when recording a video sequence, a feature will not move much from frame to frame. We will now use the most well-known differential feature tracker, also known as Lucas-Kanade (LK) Tracker. . Using OpenCV’s documentation and the C++ API for the LK tracker, track features for the video sequences we provided you by using the Harris corner detector (like here). Show the feature tracks at a given frame extracted when using the Harris corners, such as this: | Add an extra entry to the table used in Deliverable 6. using the Harris + LK tracker that you implemented. | What assumption about the features does the LK tracker rely on? | Comment on the different results you observe between the table in this section and the one you computed in the other sections. | Hint: You will need to convert the image to grayscale with cv::cvtColor and will want to look into the documentation for cv::goodFeaturesToTrack and cv::calcOpticalFlowPyrLK. The rest of the trackFeatures() function should be mostly familiar feature matching and inlier mask computation similar to the previous sections. Also note that the status vector from calcOpticalFlowPyrLK indicates the matches. . | Hint: For the show() method, you will just need to create a copy of the input frame and then make a loop that calls cv::line and cv::circle with correct arguments before calling imshow. . | . 📨 [Optional] Deliverable 8 - Optical Flow [+20 pts] . LK tracker estimates the optical flow for sparse points in the image. Alternatively, dense approaches try to estimate the optical flow for the whole image. Try to calculate your own optical flow, or the flow of a video of your choice, using Farneback’s algorithm. . Hint: Take a look at this tutorial, specifically the section on dense optical flow. Please post on piazza if you run into any issues or get stuck anywhere. | . .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab5/exercises",
    "relUrl": "/lab5/exercises"
  },
  "7": {
    "title": "Exercises",
    "content": "Exercises . Submission Individual | Team | Deadline | | 👤 Individual 📨 Deliverable 1 - Nister’s 5-point Algorithm [20 pts] Read the paper and answer the questions below | | 📨 Deliverable 2 - Designing a Minimal Solver [15 pts] | | 👥 Team Getting started: code base and datasets | Let’s perform motion estimation! | 📨 Deliverable 3 - Initial Setup [5 pts] | 📨 Deliverable 4 - 2D-2D Correspondences [45 pts] 1. cameraCallback: this is the main function for this lab. | 2. evaluateRPE: evaluating the relative pose estimates | 3. Publish your relative pose estimate | | 📨 Deliverable 5 - 3D-3D Correspondences [20 pts] 1. cameraCallback: Implement Arun’s algorithm | | Performance Expectations | Summary of Team Deliverables | | Submission . In this Lab, there are 5 deliverables throughout the handout. Deliverables 1 and 2 will require pen and paper and are considered individual tasks, while Deliverable 3-5 are a team task which requires coding in the lab6 directory that we will provide. . Individual . Please push the deliverables into your personal repository, for math-related questions LaTeX (or other typesetting software) is required. . Team . Please push the source code of the entire lab6 package in the folder lab6 of the team repository. Include also in your lab6 folder a PDF containing non-code deliverables (plots, comments). . Deadline . Deadline: the VNAV staff will clone your repository on October 15 at midnight ET. . 👤 Individual . 📨 Deliverable 1 - Nister’s 5-point Algorithm [20 pts] . Read the paper and answer the questions below . Read the following paper. . [1] Nistér, David. “An efficient solution to the five-point relative pose problem.” 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Vol. 2. 2003. link here. . Questions: . Outline the main computational steps required to get the relative pose estimate (up to scale) in Nister’s 5-point algorithm. | Does the 5-point algorithm exhibit any degeneracy? (degeneracy = special arrangements of the 3D points or the camera poses under which the algorithm fails) | When used within RANSAC, what is the expected number of iterations the 5-point algorithm requires to find an outlier-free set? Hint: take same assumptions of the lecture notes | . | 📨 Deliverable 2 - Designing a Minimal Solver [15 pts] . Can you do better than Nister? Nister’s method is a minimal solver since it uses 5 point correspondences to compute the 5 degrees of freedom that define the relative pose (up to scale) between the two cameras (recall: each point induces a scalar equation). In the presence of external information (e.g., data from other sensors), we may be able use less point correspondences to compute the relative pose. . Consider a drone flying in an unknown environment, and equipped with a camera and an Inertial Measurement Unit (IMU). We want to use the feature correspondences extracted in the images captured at two consecutive time instants $t_1$ and $t_2$ to estimate the elative pose (up to scale) between the pose at time $t_1$ and the pose at time $t_2$. Besides the camera, we can use the IMU (and in particular the gyroscopes in the IMU) to estimate the relative rotation between the pose of the camera at time $t_1$ and $t_2$. . You are required to solve the following problems: . Assume the relative camera rotation between time and is known from the IMU. Design a minimal solver that computes the remaining degrees of freedom of the relative pose. Hint: we only want to compute the pose up to scale | . | OPTIONAL (5 bonus pts): Describe the pseudo-code of a RANSAC algorithm using the minimal solver developed in point a) to compute the relative pose in presence of outliers (wrong correspondences). | 👥 Team . In this section, we will estimate the motion of a (simulated) flying drone in real time and compare the performances of different algorithms. . For the algorithms, we will be using the implementations provided in the OpenGV library (note: OpenGV). . For the datasets, we will use pre-recorded rosbag files of our simulated drone flying in an indoor environment. . Additionally, for motion estimation: . We will only focus on two-view (vs multi-camera) pose estimation. In OpenGV, we refer to two-view problems as “Central” (vs “Non-Central”) relative pose problems. | We will focus only on the calibrated case, where the intrinsics matrix K is given, and we assume that the images are rectified (distortion removed) using the parameters that you estimated previously. | . Getting started: code base and datasets . Prerequisites: Lab 6 will use the feature matching algorithms developed in Lab 5 (in particular, we use SIFT matching), so make sure you have a working version of Lab 5 already in the VNAV workspace. . | Prepare the code base: Use git pull to update the git repo used to distribute lab codes (https://github.mit.edu/VNAV2020/labs), and you should see a new folder named lab6. Copy the entire lab6 folder to the src folder of your vnav workspace (e.g., ~/vnav_ws/src). Now we are ready to install OpenGV by doing: cd ~/vnav_ws/src (path to src of vnav workspace) wstool init wstool merge lab6/install/lab_6.rosinstall -y wstool update -j8 . The above scripts will download OpenGV into your workspace (you will see a folder opengv under src). Now run: . catkin build lab_6 . to build the lab_6 package (which should build OpenGV first and then build lab_6 itself). . | Download the datasets: We will use the following dataset for this lab: office.bag and you can download it here. | | . After downloading the dataset, we suggest you to put them in the ~/data/vnav folder. . The rosbag files include the following topics of the drone: . Ground-truth pose estimate of the drone’s body frame: /tesse/odom | RGB image from the left-front camera of the drone: /tesse/left_cam/rgb/image_raw | Depth image: /tesse/depth_cam/mono/image_raw | . You can play these datasets by running (after using roscore to start ROS master first): . rosbag play ~/data/vnav/office.bag . while in parallel open RVIZ by: . rviz -d ~/vnav_ws/src/lab6/rviz/office.rviz . You should see on the left the RGB Image and the Depth image. . Let’s perform motion estimation! . We will use two methods to estimate the motion of the drone: . Motion estimation from 2D-2D correspondences (Deliverable 4) | Motion estimation from 3D-3D correspondences (Deliverable 5) | . In Deliverable 4, we will perform motion estimation only using 2D RGB images taken from the drone’s camera, while in Deliverable 5, we will additionally use the depth measurements to get the sense of 3D. . NOTE: . All your main implementations of the motion estimation algorithms should be in the pose_estimation.cpp file. In the file, we have also provided many comments to help your implementation, so please go through the comments in details. | For this lab, we provide a number of useful utility functions in lab6_utils.h. You do not need to use these functions to complete the assignment, but they might help save you some time and frustration. | . 📨 Deliverable 3 - Initial Setup [5 pts] . Before we go to motion estimation, an important task is to calibrate the camera of the drone, i.e., to obtain the camera intrinsics and distortion coefficients. Normally you would need to calibrate the camera yourself offline to obtain the parameters. . However, in this lab the camera that the drone is equipped with has been calibrated already, and calibration information is provided to you! (If you are curious about how to calibrate a camera, feel free to check this ROS package) . As part of the starter code, we provide a function calibrateKeypoints to calibrate and undistort the keypoints. Make sure you use this function to calibrate the keypoints before passing them to RANSAC. . 📨 Deliverable 4 - 2D-2D Correspondences [45 pts] . Given a set of keypoint correspondences in a pair of images (2D - 2D image correspondences), as computed in the previous lab 5, we can use 2-view (geometric verification) algorithms to estimate the relative pose (up to scale) from one viewpoint to another. . To do so, we will be using three different algorithms and comparing their performance. . We will first start with the 5-point algorithm of Nister. Then we will test the 8-point method we have seen in class. Finally, we will test the 2-point method you developed in Deliverable 2. For all techniques, we use the feature matching code we developed in Lab 5 (use the provided solution code for lab 5 if you prefer - download it here). In particular, we use SIFT for feature matching in the remaining of this problem set. . We provide you with a skeleton code in lab6 folder where we have set-up ROS callbacks to receive the necessary information. . We ask you to complete the code inside the following functions: . 1. cameraCallback: this is the main function for this lab. . Inside, you will have to use three different algorithms to estimate the relative pose from frame to frame: . OpenGV’s the 5-point algorithm with RANSAC (see OpenGV API) | OpenGV’s 8-point algorithm by Longuet-Higgins with RANSAC | OpenGV’s 2-point algorithm with RANSAC. This algorithm requires you to provide the relative rotation between pairs of frames. This is usually done by integrating the IMU’s gyroscope measurements. Nevertheless, for this lab, we will ask you to compute the relative rotation using the ground-truth pose of the drone between both frames. | . For each part, follow the comments written in the source code for further details. . We strongly recommend you to take a look at how to use OpenGV functions here. . OPTIONAL (5 bonus pts): if you are curious about how important is to reject outliers via RANSAC, try to use the 5-point method without RANSAC (see OpenGV API), and add the results to the performance evaluation below. . 2. evaluateRPE: evaluating the relative pose estimates . After implementing the relative pose estimation methods, you are required to evaluate their accuracy and plot their errors over time. Since you also have the ground-truth pose of the drone, it is possible to compute the Relative Pose Error (RPE) between your estimated relative pose from frame to frame and the actual ground-truth movement. Follow the equations below and compute the translation and rotation relative errors on the rosbag we provided. . The relative pose error is a metric for investigating the local consistency of a trajectory . RPE compares the relative poses along the estimated and the reference trajectory. Given the ground truth pose $T^W_{ref,t}$ at time $t$ (with respect to the world frame $W$), we can compute the ground truth relative pose between time $t-1$ and $t$ as: . [ T_{ref,t}^{ref,t-1} = left(T^W_{ref,t-1} right)^{-1} T^W_{ref,t} in SE{3} ] . Similarly, the 2-view geometry algorithms we test in this lab will provide an estimate for the relative pose between the frame at time $t-1$ and $t$: . [ T^{est,t-1}_{est,t} in SE{3} ] . Therefore, we can compute the mismatch between the ground truth and the estimated relative poses using one of the distances we discussed during lecture. . When using 2D-2D correspondences, the translation is only computed up to scale (and is conventionally returned as a vector with unit norm). so we recommend scaling the corresponding ground truth translation to have unit norm before computing the errors we describe below. . Relative translation error: This is simply the Euclidean distance between the ground truth and the estimated relative translation: . [ RPE_{t-1,t}^{tran} = left Vert mathrm{trans} left(T_{ref,t}^{ref,t-1} right) - mathrm{trans} left(T^{est,t-1}_{est,t} right) right Vert_2 ] . where $ mathrm{trans}( cdot)$ denotes the translation part of a pose. . Relative rotation error: This is the chordal distance between the ground truth and the estimated relative rotation: . [ RPE_{i,j}^{rot} = left Vert mathrm{rot} left(T_{ref,t}^{ref,t-1} right) - mathrm{rot} left(T_{est,t}^{est,t-1} right) right Vert_{F} ] . where $ mathrm{rot}( cdot)$ denotes the rotation part of a pose. . You will need to implement these error metrics, compute them for consecutive frames in the rosbag, and plot them as discussed above. . As a deliverable, provide 2 plots showing the rotation error and the translation error over time for each of the tested techniques (2 plots with 3 lines for the algorithms using RANSAC). You can write the data to a file and do the plotting with Python if you prefer (upload as well the python script if necessary). . 3. Publish your relative pose estimate . In order to visualize your relative pose estimate between time $t-1$ and $t$, we postmultiply your estimated relative pose between time $t-1$ and $t$ by the ground truth pose at time $t-1$. This will give you a pose estimate at time $t$ that you can visualize in Rviz. To do so, we use the ground-truth pose of the previous frame (obtained from ROS messages), “plus” the relative pose between current frame and previous frame (obtained from your algorithms, and then scale the translation using ground-truth), to compute the estimated (absolute) pose of the current frame, and then publish it. . To run your code, use: . roslaunch lab_6 video_tracking.launch . but be sure to modify the dataset path and parameters to run the correct method! For example, the pose_estimator parameter determines which algorithm to be used for the motion estimation. . Note that we are cheating in this visualization since we use the ground truth from the previous time stamp. In practice, we cannot concatenate multiple estimates from 2-view geometry since they are up to scale (so for visualization, we use groundtruth to recover the scale). . In the next deliverable we will see that 3D-3D correspondences allow us to reconstruct the correct scale for the translation.** . 📨 Deliverable 5 - 3D-3D Correspondences [20 pts] . The rosbag we provide you also contains depth values registered with the RGB camera, this means that each pixel location in the RGB camera has an associated depth value in the Depth image. . In this part, we have provided code to scale to bearing vectors to 3D point clouds, and what you need to do is to use Arun’s algorithm (with RANSAC) to compute the drone’s relative pose from frame to frame. . 1. cameraCallback: Implement Arun’s algorithm . Implement Arun’s algorithm in this function. Use the evaluateRPE function you used previously to plot the rotation error and the translation error over time as well. Mind that, in this case, there is no scale ambiguity, therefore we cannot really compare the translation error of this approach against the previous ones. Implement Arun’s algorithm with RANSAC using OpenGV. . To run your code, use: . roslaunch lab_6 video_tracking.launch . with the pose_estimator parameter set to 3 so that Arun’s method is used. . Note that while we can now reconstruct the trajectory by concatenating the relative poses, such a trajectory estimate will quickly diverge due to error accumulation. In future lectures, we will study Visual-Odometry and Loop closure detection as two ways to mitigate the error accumulation. . Performance Expectations . What levels of rotation and translation errors should one expect from using these different algorithms? To set the correct expection, we think the following errors are satisfactory: . Using 5-point or 8-pt with RANSAC, for most of the frames, you can get rotation error below 1 degree and translation error below 0.5 (note that the translation error is between 0 and 2 since both ground-truth translation and estimated translation have unit norm), with 5-pt algorithm slightly outperforming 8-pt algorithm. | Using 2-point with RANSAC, for most of the frames, you can get the translation error below 0.1 (note that the translation error is between 0 and 2). | Using 3-point with RANSAC (3D-3D), for most of the frames, you can get rotation error below 0.1 degree, and translation error below 0.1 (if you normalize the translations), and even smaller if you don’t normalize the translations since the frame rate is very high. | . Summary of Team Deliverables . For the given dataset, we require you to run all algorithms on it and compare their performances. Therefore, as a summary for Team Deliverables: . Plots of translation and rotation error for each of the methods (5pt, 8pt, 2pt, Arun 3 pt) using the given rosbag (using RANSAC is required, while without RANSAC is optional). | OPTIONAL (10 bonus pts): repeat the tests using a rosbag you collect during drone racing (your Lab 4 solution). The rosbag must contain stereo RGB images, depth information, and odometry, which are not published by tesse_ros_bridge by default. So, in order to collect a suitable rosbag, you must follow these steps: Download the updated simulator and run it with these command line arguments: -screen-width 720 -screen-height 480 | Launch the tesse ros bridge with these arguments: roslaunch tesse_ros_bridge tesse_quadrotor_bridge.launch publish_stereo_rgb:=true publish_depth:=true publish_odom:=true | Collect the rosbag of your trajectory as before (e.g. by launching the appropriate nodes and running rosbag record -a in another window) | If you experience any errors following the above steps, please check Piazza and make a new post if your question is not already answered. | |",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab6/exercises",
    "relUrl": "/lab6/exercises"
  },
  "8": {
    "title": "Git",
    "content": "Git . . Intro | Getting started | Merge Conflict with an Imaginary Collaborator | Intro . The following are selected chapters from Pro Git, if you are new to Git please read carefully these chapters as they pose the foundation of git. . Getting Started | Getting a Git Repository | Recording Changes to the Repository | Viewing the Commit History | Undoing Things | Working with Remotes | Branches in a Nutshell | Basic Branching and Merging | Branch Management | Branching Workflows | Remote Branches | Also visit Git command reference to get help with commands and command syntax. . . The following exercises are designed to help you to experiment and learn how these commands are used in practice. . Getting started . Install Git with (requires internet connection). For this you need to update APT cache and then install the git-core package | Add your name/email to your Git configuration (system-wide) | git config --global user.name YOUR_NAME git config --global user.email YOUR_MIT_EMAIL_ADDRESS . Generate SSH keys (do not forget the passphrase if you choose to set one) | Add SSH keys to your github.mit.edu account | Create a new repository on https://github.mit.edu | Open a terminal and create a new directory using mkdir named vnav19 in your HOME directory | Clone your (empty) Git repo (earn street cred by calling by using “repo” instead of “repository”) | git clone git@github.mit.edu:USERNAME/REPO.git . Merge Conflict with an Imaginary Collaborator . Now we simulate a common situation that arises when two or more people use the same repo. . Navigate to your repo and create new me.txt with your name and MIT email, e.g. $ cat me.txt Jon Snow lordsnow@mit.edu . | Check the status with git status | Add (stage), check the status, commit and push your changes – commit message can be “Added my email” add me.txt git status git commit -m &quot;Added my email&quot; git push . | Inspect the log with git log | Now you can go to your repo’s page on Github and inspect the commit history and contents of your repo. . Let’s continue editing the files . Let’s create a new branch git checkout -b new_branch_to_merge_later . | Edit the file me.txt with completely different content, e.g. $ cat me.txt Arya Stark astark@mit.edy . | Add (stage), check the status, and commit your changes (you can push too if you want) – commit message can be “Somebody added another email” | Now switch branch to master with git checkout master | Inspect the output of git log --graph --oneline --all | Append your course number to the file $ echo &quot;Course 16&quot; &gt;&gt; me.txt $ cat me.txt Jon Snow lordsnow@mit.edu Course 16 . | Add (stage), check the status, and commit your changes – commit message can be “Added my course number” | Merge the two branches $ git merge new_branch_to_merge_later Auto-merging me.txt CONFLICT (content): Merge conflict in me.txt Automatic merge failed; fix conflicts and then commit the result. . | BOOM 💥. A conflict appears. Thanks, Git for letting us know about this! Let’s resolve the conflict . Inspect the file me.txt, you should see something like - Git helps us by marking the conflict region with special characters: HEAD refers to your current branch/commit and below the ======= the other commit $ cat me.txt &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Jon Snow lordsnow@mit.edu Course 16 ======= Arya Stark astark@mit.edu &gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_to_merge_later . | In this case, we would like to have Jon name so we simply remove everything else (including &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD) from the file | After resolving the conflict, it is time to stage our file and create our merge commit - inspect the log, see the diff, and check the status git add me.txt git commit -m &quot;Merge commit&quot; git push . | Inspect the output of git log --graph --oneline --all | Inspect the output of git diff HEAD~2 - what does this command do? |",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/git",
    "relUrl": "/lab1/git"
  },
  "9": {
    "title": "How to print",
    "content": "How to print . To print a page of this website it is suggested to enable the reader before printing it . Safari: Shift+CMD+R | Firefox Widnows: CTRL+Alt+R | Mac OS: CMD+Alt+R | . | Chrome (should work directly with print) | .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/about",
    "relUrl": "/about"
  },
  "10": {
    "title": "Home",
    "content": "16.485 - Visual Navigation for Autonomous Vehicles (2020) . . Learn about the mathematical foundations of visual navigation (spanning from geometry to optimization), state-of-the-art algorithms, and software packages. . In this pages you will find tutorials and exercises from the class. .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/",
    "relUrl": "/"
  },
  "11": {
    "title": "Installing ROS",
    "content": ". Installing ROS . . Getting started with ROS | Installing ROS Setup repositories | Installation | Environment setup | Dependencies for building packages | Initialization | | Getting started with ROS . The Robot Operating System (ROS) is a crucial middleware (a.k.a. collection of software packages) that enables roboticists all over the world to implement their algorithms in a clean and modular fashion and share their work effectively with the community. . In addition, it is at the very core of our class, so we’d better start playing with it! . Installing ROS . By now, you should have a working (preferably fresh) install of Ubuntu 18.04 and have become accustomed with the basics of Linux, Git and C++. The most efficient way to install ros is through the Debian (binary) packages. . To install ROS Melodic on it, we will follow the official guide to install the Desktop-Full Install option. . Setup repositories . Let’s add the packages.ros.org repository to our system . sudo sh -c &#39;echo &quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&quot; &gt; /etc/apt/sources.list.d/ros-latest.list&#39; . and setup the keys . sudo apt-key adv --keyserver &#39;hkp://keyserver.ubuntu.com:80&#39; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 . Installation . Before installing ROS we need to update the apt index . sudo apt update . Now let’s install ROS 🤖 . sudo apt install ros-melodic-desktop-full . Note: if you encounter the following problem when running the command line above: . Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: ros-melodic-desktop-full : Depends: ros-melodic-perception but it is not going to be installed E: Unable to correct problems, you have held broken packages. . Then a solution is to instead of running sudo apt install ros-melodic-desktop-full, perform the following: . sudo apt install aptitude sudo aptitude install ros-melodic-desktop-full . Environment setup . It’s convenient if the ROS environment variables are automatically loaded as soon a new shell is launched, let’s edit ~/.bashrc to do so . echo &quot;source /opt/ros/melodic/setup.bash&quot; &gt;&gt; ~/.bashrc source ~/.bashrc . Dependencies for building packages . Up to now you have installed what you need to run the core ROS packages. To create and manage your own ROS workspaces, there are various tools and requirements that are distributed separately. For example, rosinstall is a frequently used command-line tool that enables you to easily download many source trees for ROS packages with one command. . To install this tool and other dependencies for building ROS packages, run: . sudo apt install python-rosdep python-rosinstall python-rosinstall-generator python-wstool build-essential python-catkin-tools rosbash . Initialization . Before using ROS, we need to initialize rosdep. . sudo rosdep init rosdep update .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab2/ros",
    "relUrl": "/lab2/ros"
  },
  "12": {
    "title": "Introduction to ROS",
    "content": "ROS Intro . . ROS file system structure General structure | The workspace | | ROS Master, nodes and topics ROS Master | ROS nodes | ROS topics | | Anatomy of a ROS node | Launch files | Transforms (tf package) Quick overview of tf tools Using view_frames | Using rqt_tf_tree | Using tf_echo | Usign RViz | | | Additional resources | ROS file system structure . General structure . Similar to an operating system, ROS files are also organized in a particular fashion. The following graph shows how ROS files and folder are organized on the disk: . . The ROS packages are the most basic unit of the ROS software. They contain the ROS runtime process (nodes), libraries, configuration files, and so on, which are organized together as a single unit. Packages are the atomic build item and release item in the ROS software. . Inside a package we can find the package manifest file, which contains information about the package, author, license, dependencies, compilation flags, and so on. The package.xml file inside the ROS package is the manifest file of that package. . The ROS messages are a type of information that is sent from one ROS process to the other. They are regular text files with .msg extension that define the fields of the messages. . The ROS service is a kind of request/reply interaction between processes. The reply and request data types can be defined inside the srv folder inside the package. . For example, the package we will develop in this lab will be like . . └── two_drones_pkg ├── CMakeLists.txt ├── README.md ├── config │   └── default.rviz ├── launch │   └── two_drones.launch ├── mesh │   └── quadrotor.dae ├── package.xml └── src ├── frames_publisher_node.cpp └── plots_publisher_node.cpp . The workspace . In general terms, the workspace is a folder which contains packages, those packages contain our source files and the environment or workspace provides us with a way to compile those packages. It is useful when we want to compile various packages at the same time and it is a good way of centralizing all of our developments. . ROS Master, nodes and topics . One of the primary purposes of ROS is to facilitate communication between the ROS modules called nodes. Those nodes can be executed on a single machine or across several machines, obtaining a distributed system. The advantage of this structure is that each node can control one aspect of a system. For example you might have several nodes each be responsible of parsing row data from sensors and one node to process them. . ROS Master . Communication between nodes is established by the ROS Master. The ROS Master provides naming and registration services to the nodes in the ROS system. It is its job to track publishers and subscribers to the topics. . ROS master works much like a DNS server. Whenever any node starts in the ROS system, it will start looking for ROS master and register the name of the node with ROS master. Therefore, ROS master has information about all the nodes that are currently running on the ROS system. When information about any node changes, it will generate a call back and update with the latest information. . . ROS Master distributes the information about the topics to the nodes. Before a node can publish to a topic, it sends the details of the topic, such as its name and data type, to ROS master. ROS master will check whether any other nodes are subscribed to the same topic. If any nodes are subscribed to the same topic, ROS master will share the node details of the publisher to the subscriber node. . After receiving the node details, these two nodes will interconnect using the TCPROS protocol, which is based on TCP/IP sockets, and ROS master will relinquish its role in controlling them. . To start ROS master, open a terminal and run . roscore . Any ROS system must have only one master, even in a distributed system, and it should run on a computer that is reachable by all other computers to ensure that remote ROS nodes can access the master. . ROS nodes . Basically, nodes are regular processes but with the capability to register with the ROS Master node and communicate with other nodes in the system. The ROS design idea is that each node is an independent module that interacts with other nodes using the ROS communication capability. . The nodes can be created in various ways. From a terminal window a node can be created directly by typing a command after the command prompt, as shown in the examples to follow. Alternatively, nodes can be created as part of a program written in Python or C++. In this lab, we will use both the ROS commands in a terminal window and Python programs to create nodes. . As example let’s run the turtlesim node, in a new terminal run . rosrun turtlesim turtlesim_node . Yous should see something like . . Now, you can ask the ROS master about the running nodes with . $ rosnode list /rosout /turtlesim . Now, let’s run another node . rosrun turtlesim turtle_teleop_key . Now the list of node changed . $ rosnode list /rosout /teleop_turtle /turtlesim . ROS topics . Topics are the means used by nodes to transmit data, it represents the channel where messages are sent and it has a message type attached to it (you cannot send different types of messages in a topic). In ROS, data production and consumption are decoupled, this means that a node can publish message (producer) or subscribe to a topic (consumer). . Let’s use rqt_graph which shows the nodes and topics currently running. . rosrun rqt_graph rqt_graph . If you select Nodes/Topics (all) from the top left and deselect Debug you will see something similar to . . In the graph the ellipses are nodes and the squares are topics. From the picture it’s easy to see that teleop_turtle is publishing to /turtle1/cmd_vel topic. The node /turtlesim is subscribed to the topic and uses the incoming messages to move the turtle. . You can also print the messages to the terminal. Try tu run rostopic echo /turtle1/cmd_vel and move the turtle, you should get something like . $ rostopic echo /turtle1/cmd_vel linear: x: 0.0 y: 0.0 z: 0.0 angular: x: 0.0 y: 0.0 z: -2.0 . These are very useful tools to debug your nodes. . Anatomy of a ROS node . The simplest C++ ROS node has a structure similar to the following . #include &quot;ros/ros.h&quot; int main(int argc, char **argv) { ros::init(argc, argv, &quot;example_node&quot;); ros::NodeHandle n; ros::Rate loop_rate(50); while (ros::ok()) { // ... do some useful things ... ros::spinOnce(); loop_rate.sleep(); } return 0; } . Let’s analyze it line by line: the first line . #include &quot;ros/ros.h&quot; . adds the header containing all the basic ROS functionality. At the beginning of the main of the program . ros::init(argc, argv, &quot;example_node&quot;); . ros::init initialize the node, it is responsible for collecting ROS specific information from arguments passed at the command line and set the node name (remember: names must be unique across the ROS system). But it does not contact the master. To contact the master and register the node we need to call . ros::NodeHandle n; . When the first ros::NodeHandle is created it will call ros::start(), and when the last ros::NodeHandle is destroyed (e.g. goes out of scope), it will call ros::shutdown(). This is the most common way of handling the lifetime of a ROS node. . Usually we want to run our node at a given frequency, to set the node frequency we use . ros::Rate loop_rate(50); . which is setting the desired rate at 50 Hz. Than we have the main loop of the node. Since we want to run this node until the ROS we need to the check the various states of shutdown. The most common way to do it is to call ros::ok(). Once ros::ok() returns false, the node has finished shutting down. That’s why we have . while (ros::ok()) { // ... } . Inside the loop we can make interesting things happen. In our example we simply run . ros::spinOnce(); loop_rate.sleep(); . The function ros::spinOnce() will call all the callbacks waiting to be called at that point in time while. If you remember we set the node frequency to 50Hz, the code we are running will probably take less than 20ms. The function loop_rate.sleep() will pause the node the remaining time. . Launch files . Launch files are the preferred way to run ROS nodes. The launch files provide a convenient interface to execute multiple nodes and a master (if is not already running), as well as other initialization requirements such as parameters. . Usually the launch files are located in the launch folder of the package and have .launch extension. If the package provide one you can use roslaunch to use it. . roslaunch &lt;package_name&gt; &lt;launch_file&gt; . Note: Pushing CTRL+c in a terminal with a launch file running will close all nodes that were started with that launch files. . An example of launch file is . &lt;launch&gt; &lt;node name=&quot;map_server&quot; pkg=&quot;map_server&quot; type=&quot;mapserver&quot; /&gt; &lt;node name=&quot;stageros&quot; pkg=&quot;stage&quot; type=&quot;stageros&quot; args=&quot;$(find navigation_stage)/stage config/worlds/willow-pr2-5cm.world&quot; &gt; &lt;param name=&quot;base_watchdog_timeout&quot; value=&quot;0.2&quot;/&gt; &lt;/node&gt; &lt;include file=&quot;$(find navigation_stage)/move_base_config/amcl_node.xml&quot;/&gt; &lt;/launch&gt; . This example will run three nodes (plus the master if not already running), each &lt;node&gt;...&lt;/node&gt; is equivalent to a rosrun call, for example . &lt;node name=&quot;stageros&quot; pkg=&quot;stage&quot; type=&quot;stageros&quot; args=&quot;$(find navigation_stage)/stage_config/worlds/willow-pr2-5cm.world&quot; /&gt; . is equivalent to . rosrun stage stageros &lt;path-to-navigation-stage-package&gt;/stage_config/worlds/willow-pr2-5cm.world . This tool gives the possibility to add complex and dynamic runtime behavior such as $(find path), unless and if or include other launch files. . Transforms (tf package) . The ROS tf library has been developed to provide a standard method to keep track of coordinate frames and transform data within the entire system so that users can be confident about the consistency of their data in a particular coordinate frame without requiring knowledge about all the other coordinate frames in the system and their associations. . tf is distributed across nodes (across machines too, eventually) and there are two types of tf nodes . Listener: that listen to /tf and cache all data that it collected (up to cache limit) | Broadcaster: that publish transforms between coordinate frames on /tf | . In tf, transforms and coordinate frames are represented as a graph with the transforms as edges and the coordinate frames as nodes. The advantage of this representation is that the relative pose between two nodes is simply the product of the edges connecting the two nodes. A tree structure has also the benefit of allowing for dynamic changes easily. tf indeed takes care of ambiguity of transforms not allowing loops in the transforms graph. . Where to learn TF. . The best resource to learn TF out there is the official ROS tf tutorials. . Take your time to familiarize with the Listener/Broadcaster code, you&#39;ll need for the exercises. . Of course do not forget the official documentation. . Quick overview of tf tools . Let’s see the tools we have to explore the tf tree. . ROS provides a simple demo we are going to use, from a terminal run . roslaunch turtle_tf turtle_tf_demo.launch . Once the turtlesim demo is started, we will drive the center turtle around in turtlesim using the keyboard arrow keys. We can observe that one turtle will continuously follow the turtle we are driving. In this demo application, the ROS TF library is used to create three coordinate frames: a world frame, a turtle1 frame, and a turtle2 frame, and to create a TF broadcaster to publish the coordinate frames of the first turtle and a TF listener to compute the difference between the first and follower turtle frames, as well as drive the second turtle to follow the first. . Using view_frames . The view_frames tool creates a diagram of the frames being broadcast by TF over ROS: . rosrun tf view_frames evince frames.pdf . . Here, you can see that three frames are broadcast by TF—the world, turtle1, and turtle2, where the world frame is the parent of the turtle1 and turtle2 frames. . Using rqt_tf_tree . The rqt_tf_tree tool enables the real-time visualization of the tree of frames being broadcast over ROS . rosrun rqt_tf_tree rqt_tf_tree . . Using tf_echo . The tf_echo tool reports the transformation between any two frames broadcast over ROS . rosrun tf tf_echo [reference_frame] [target_frame] . For example . $ rosrun tf tf_echo turtle1 turtle2 At time 1568050753.324 - Translation: [0.000, 0.000, 0.000] - Rotation: in Quaternion [0.000, 0.000, 0.344, 0.939] in RPY (radian) [0.000, -0.000, 0.703] in RPY (degree) [0.000, -0.000, 40.275] . Usign RViz . RViz is a graphical 3D visualization tool that is useful for viewing the association between TF frames within the ROS system: . . You can run RViz just running . rviz . One it is started, you need to press the button Add to add the TF in the visualization and set the correct Fixed Frame (i.e. world). . Additional resources . Of course there are many details we haven’t discussed here, below you can find resources we find interesting . Official ROS tutorial | A Gentle Introduction to ROS | Free videos from Robot Ignite Academy | Programming Robots with ROS | .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab2/ros101",
    "relUrl": "/lab2/ros101"
  },
  "13": {
    "title": "Lab 1",
    "content": "Lab 1 . In this lab we will install Ubuntu 18.04, learns the shell basics and write our first C++ programs. .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/",
    "relUrl": "/lab1/"
  },
  "14": {
    "title": "Lab 2",
    "content": "Lab 2 . In this lab we will install ROS and will experiment with tf and Homogeneous Transformations. .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab2/",
    "relUrl": "/lab2/"
  },
  "15": {
    "title": "Lab 3",
    "content": "Lab 3 .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab3/",
    "relUrl": "/lab3/"
  },
  "16": {
    "title": "Lab 4",
    "content": "Lab 4 .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab4/",
    "relUrl": "/lab4/"
  },
  "17": {
    "title": "Lab 5",
    "content": "Lab 5 . This lab consists of: . Part 1 [individual]: Theoretical questions | Part 2 [team]: Get familiar with state-of-the-art feature tracking techniques | . In Part 1, we will deal with theory related to perspective projection. In Part 2, we will have you try different feature tracking algorithms that are the basis for many computer vision algorithms. . If you’re having trouble or encounter problems during the implementation, feel free to use Piazza to ask questions so your fellow classmates can respond and benefit from the answers as well. .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab5/",
    "relUrl": "/lab5/"
  },
  "18": {
    "title": "Lab 6",
    "content": "Lab 6 . Welcome to VNAV Lab 6! This lab consists of the following parts: . Part 1 [individual]: Theoretical questions. . Part 2 [team]: Motion estimation of a drone using different algorithms. . In Part 1, we will test your knowledge on some of the theory of 2-view geometry. In Part 2, we will have you use different vision-based pose estimation algorithms to estimate the motion of a flying drone (we provide rosbag datasets). . If you struggle finding a solution or encounter problems/doubts during the coding parts, please use​ Piazza to ask questions so your fellow classmates can reuse the answers as well. .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab6/",
    "relUrl": "/lab6/"
  },
  "19": {
    "title": "Shell basics",
    "content": "Shell basics . . Exploring the Filesystem pwd | ls File permissions and ownership | | cd | find | | Edit Filesystem mkdir | touch | rm | cp | mv | | Viewing and Editing Files cat | nano and vim | | Download, uncompress and execute a script Download | Uncompress | Run | | Pipe | Output redirect | Superuser | Install packages | Get help | Exploring the Filesystem . pwd . Modern filesystems are organized in folders, being able to navigate the filesystem is fundamental. Everytime we work with the shell we are within one folder, to know where we are we can use the command pwd (Print Working Directory): . $ pwd /home/username/vnav2020/lab1 . where username is the login username you set. . ls . To list the contents of the current directory (files and/or child directories, etc.) we use ls (LiSt) . $ ls ex0.cpp ex1.cpp ex2.cpp final.cpp . File permissions and ownership . The concept of permissions and ownership is crucial in anu unix system. Every to file and directory is assigned 3 types of owner: . User: is the owner of the file, by default, the person who created a file | Group: user-group can contain multiple users, all users belonging to a group will have the same access permissions to the file | Other: Any other user who has access to a file | . At the same time to every file and directory is assigned a type of permission . Read | Write | Execute | . We get all this information using ls -l, for example: . $ ls -l total 1112 -rw-r--r-- 1 username staff 557042 Aug 24 21:57 dante.txt -rwxr-xr-x 1 username staff 40 Aug 23 18:36 hello.sh -rw-r--r-- 1 username staff 171 Aug 23 18:28 hello_vnav2020.tar.gz -rw-r--r-- 1 username staff 49 Aug 24 22:55 numbers.txt . The permissions are specified by the 1st field, the ownership is specified by the 3rd and 4th fields. Fo example, the file hello.sh is owned by me (username) and the group is staff. THe permission string is -rwxr-xr-x meaning that: . The owner can read (r), write (w) and execute (x) the file | The group can read and execute | Other can read and execute | . cd . To change the current folder we can use cd (Change Directory). For example cd / moves to the file system root or . cd /home . To move to the parent of the current folder we use cd .., it can also be concatenated like cd ../.. to move two (or more) levels up. To move back to your home folder we use cd ~ (or simply cd). . find . Image you have a folder containing many files and you want to locate a file called findme.txt. To accomplish it you can use . find . -name &quot;findme.txt&quot; . Let’s analyze the command. The . represent the current folder, so we are saying to find to look in the current folder recursively (you can change it with relative or absolute paths) for a file called findme.txt. Find is a powerful tool, you can have complex expression to match files, have a look at find --help. . Edit Filesystem . mkdir . mkdir (make directory) is used to create new, empty directories: let’s create a new dir named newdir . $ mkdir newdir $ ls newdir $ cd newdir . touch . touch was created to modify file timestamps, but it can also be used to quickly create an empty file. You can easily create a newfile.txt with . $ touch newfile.txt $ ls newfile.txt . rm . You can remove any file with rm – be careful, this is non-recoverable! I suggest to add the flag -i to prompt a confirmation message . rm -i newfile.txt rm: remove regular empty file &#39;newfile.txt&#39;? y . You can also remove directories with rm, the only catch is that it returns an error when the folder is not-empty. The common practice, but pretty prone to non-recoverable errors, is to run rm -rf foldername. The command will remove the folder with all its content (r - recursive) forcing the operation (f - force). This operation will not ask for confirmation. You can of course add the flag i (i.e. rm -rfi foldername) but will ask confirmation for every file, this is pretty annoying if the folder contains many files. . cp . Copying file is as simple as running cp (CoPy). If we want to duplicate the file numbers.txt we can run . $ cp numbers.txt numbers_copy.txt $ls numbers.txt numbers_copy.txt . mv . If we want to rename numbers_copy.txt to new_numbers.txt we can run . $ mv numbers_copy.txt new_numbers.txt $ ls new_numbers.txt numbers.txt . With the same command we can also move the file to another location, for example if we want to move numbers.txt to a newly create folder dataset we execute . $ mkdir dataset $ mv numbers.txt dataset/numbers.txt $ ls dataset numbers.txt . Viewing and Editing Files . cat . cat concatenates a list of files and sends them to the standard output stream and is often used to quickly view the content of a file. For example we can inspect the content of the file numbers.txt. . $ cat numbers.txt One Two Three Four Five Six Seven Eight Nine Ten . nano and vim . nano is a minimalistic command-line text editor. It’s a great editor for beginners. More demanding user pefer vim. It’s a powerful and highly customizable text editor (I love it!). I strongly suggest to learn how to use vim, one of the best way to learn vim is to simply run vimtutor in your terminal but if you prefer games try Vim Adventures! . Download, uncompress and execute a script . In this section we will download a compressed file, extract the content, inspect and run a script. . Download . Imagine you have to download (you have to, actually) http://www.mit.edu/~username/hello_vnav19.tar.gz you can use wget. . wget http://www.mit.edu/~username/hello_vnav19.tar.gz . Uncompress . As you see it is a compressed file, to uncompress it we can use tar . tar -xvf hello_vnav2020.tar.gz . The flags xvf are respectively extract, verbose file. Now we would like to run the script. We should first inspect the file (never run a script without inspection), let’s use cat . $ cat hello.sh #!/usr/bin/env bash echo &quot;Hello world!&quot; . The file is not dangerous, it only print something to the terminal. . Run . Before running the script we should verify that we can actually run the script so let’s see its permissions . $ ls -l hello.sh -rw-r--r-- 1 username staff 40 Aug 23 18:36 hello.sh . Ops! This time we have no right to run the script, we have to add it: . chmod +x hello.sh . Let’s check again: . ls -l hello.sh -rwxr-xr-x 1 username staff 40 Aug 23 18:36 hello.sh . Ok, now we can execute the script. To execute the script it’s enough to add ./ before the name of the file to e . $ ./hello.sh Hello world! . Keep in mind. . When you use ./ the bash shell is creating a new shell, child of the current one and executing the code there. This is usually fine, sometimes you need to run a script like it was prompted directly in the current bash, in that case you should use the source command, e.g. source hello.sh . Pipe . The Pipe is a command in Linux that lets you use two or more commands such that output of one command serves as input to the next. In short, the output of each process directly as input to the next one like a pipeline. The symbol | denotes a pipe. . For example, consider the following file: . $ cat numbers.txt One Two Three Four Five Six Seven Eight Nine Ten . We can sort the lines piping cat with sort . $ cat numbers.txt | sort Eight Five Four Nine One Seven Six Ten Three Two . Output redirect . We redirect the output of a command to a file. This is useful when we want to save the output of a program without writing specific code. . The common commands that we use and their results are . command &gt; output.txt | . The standard output stream will be redirected to the file only, it will not be visible in the terminal. If the file already exists, it gets overwritten. . command &amp;&gt; output.txt | . Both the standard output and standard error stream will be redirected to the file only, nothing will be visible in the terminal. If the file already exists, it gets overwritten. . command | tee output.txt | . The standard output stream will be copied to the file, it will still be visible in the terminal. If the file already exists, it gets overwritten. . command |&amp; tee output.txt | . Both the standard output and standard error streams will be copied to the file while still being visible in the terminal. If the file already exists, it gets overwritten. . Moreover. . If you want to append instead of overwrite you can use the double angle brackets &gt;&gt;. With tee instead add the flag -a (e.g. tee -a output.txt). . Superuser . Working with the terminal you wil, sooner or later, get a “Permission denied” error. This occur because you do not have the right permission to run the command. . For example if you try install vim you might get something like . $ apt install sl E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root? . The superuser (usually “root”) is the only person who can install software, to install vim we must elevate ourself to system administrator . The command we need to use is sudo . $ sudo apt install sl [sudo] password for username: Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: sl 0 upgraded, 1 newly installed, 0 to remove and 2 not upgraded. Need to get 26.4 kB of archives. After this operation, 98.3 kB of additional disk space will be used. Get:1 http://us.archive.ubuntu.com/ubuntu bionic/universe amd64 sl amd64 3.03-17build2 [26.4 kB] Fetched 26.4 kB in 0s (250 kB/s) Selecting previously unselected package sl. (Reading database ... 162980 files and directories currently installed.) Preparing to unpack .../sl_3.03-17build2_amd64.deb ... Unpacking sl (3.03-17build2) ... Setting up sl (3.03-17build2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... . Install packages . Ubuntu, like any other Linux distribution based on Debian, use the dpkg packaging system. A packaging system is a way to provide programs and applications for installation. This way, we don’t have to build every program from the source. . APT (Advanced Package Tool) is the command line tool to interact with the packaging system. Installing a package that is available on one of the repository known by the system is as easy as running . sudo apt install &lt;package_1&gt; &lt;package_2&gt; &lt;package_3&gt; . For example if we want to install the package sl we would simply run . sudo apt install sl . Try to run sl now! . Get help . It’s hard remember all commands with all their flags, man command in Linux is used to display the user manual of any command that we can run on the terminal. . Moreover many commands offer an help (for example try to run ls --help). The common ways to summon the help is via the flags -h or --help. . Last but not least, Google is your friend! .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/shell",
    "relUrl": "/lab1/shell"
  },
  "20": {
    "title": "Install Ubuntu 18.04",
    "content": "Install Ubuntu 18.04 . For this and the following labs, you need a (preferably clean) Ubuntu 18.04 LTS (Bionic Beaver) installation (see below). There are plenty of installation guides and tutorials on the web (and, in particular, on YouTube). . Steps . Download the ISO image from ubuntu.com | Create a bootable USB stick How to create a bootable USB stick on Windows | How to create a bootable USB stick on Mac OS | How to create a bootable USB stick on Ubuntu | . | Boot from USB stick and install Install Ubuntu desktop (full erase) | Install Ubuntu alongside Windows (dual boot) | . | Warning. . Partitioning can be tricky if you are installing Linux for the first time. There are plenty of guides for “dual-boot Ubuntu installation” alongside both Windows and OS X. In most cases, you would first need to shrink one of your partitions (e.g., in Windows) and create an “unallocated space” which will be used during the Ubuntu installation process; see, e.g., this guide. . Ask for help if you are unsure. . Ubuntu Setup . Once Linux is installed we need to update all the packages, to do so open a terminal (CTRL+Alt+T) and type . sudo apt update sudo apt upgrade sudo apt install build-essential cmake .",
    "url": "https://mit-spark.github.io/VNAV2020-handouts/lab1/ubuntu",
    "relUrl": "/lab1/ubuntu"
  }
  
}
